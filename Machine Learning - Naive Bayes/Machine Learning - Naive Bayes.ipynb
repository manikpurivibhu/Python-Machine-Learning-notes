{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f11069",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc13eb",
   "metadata": {},
   "source": [
    "A <b>Naive Bayes classifier</b> is a probabilistic Machine Learning model that’s used for classification task. The crux of the classifier is based on the Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2453bd",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "$$ P(A/B) = \\frac{P(B/A) * P(A)} {P(B)} $$ \n",
    "<br>\n",
    "$$ posterior proababilty = \\frac{likelihood * prior} {marginal} $$\n",
    "\n",
    "Using Bayes theorem, we can find the probability of A happening, given that B has occurred. Here, B is the evidence and A is the hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59952c5",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "\n",
    "Naive Bayes Classifier assumes that <b>each feature acts independent of each other</b> i.e. the presence of one feature is unrelated to the presence or absence of another feature, even if those features are dependent on each other.     \n",
    "In Layman terms, it beahves as if each feature contributes independently. <br>\n",
    "\n",
    "So, \"eat healthy avoid junk\" would have the same effect/weightage as \"eat junk avoid healthy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda628a",
   "metadata": {},
   "source": [
    "### Types of Naive Bayes Classifier:\n",
    "\n",
    "The different Naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $ P(\\frac{x_i}{y})$. <br>\n",
    "\n",
    "#### Gaussian Naive Bayes:\n",
    "When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution. Since the way the values are present in the dataset changes, the formula for conditional probability changes to,\n",
    "$$ \n",
    "P(\\frac{x_i}{y}) = \\frac{1}{\\sqrt{2πσ_y^2}} exp(\\frac{-(x_i-u_i)^2}{2σ_y^2})\n",
    "$$\n",
    "\n",
    "\n",
    "#### Multinomial Naive Bayes:\n",
    "This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present in the document.\n",
    "\n",
    "#### Bernoulli Naive Bayes:\n",
    "This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n",
    "\n",
    "Other popular Naive Bayesian models are **Categorical** Naive Bayes and **Complement** Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70288b30",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "- It is an easy and fast Classification algorithm. \n",
    "- When assumption of independence holds, a Naive Bayes classifier performs better compare to other sophisticated Classifcation models with less training data.\n",
    "- It also performs well in multi-class prediction.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- If categorical variable has a feature (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as *Zero Frequency*. To solve this, we can use the *smoothing technique*, such as Laplace estimation.\n",
    "- Naive Bayes assumes independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d7255",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7288f6",
   "metadata": {},
   "source": [
    "### Naive Bayes Implementation from Sctratch with Mathematical formulas (Gaussian distribution), explanation, steps and then code\n",
    "\n",
    "NOTE: Following section implements Gaussian Naive Bayes, which asumes that the features have Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd13321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da0de7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fits Machine Learning model with training data 'X' and 'y'\n",
    "        Input: Features matrix 'X' and target vector 'y', dtype=ndarray\n",
    "        '''\n",
    "        self.X, self.y = X, y\n",
    "        self.classes = np.unique(y)\n",
    "        \n",
    "        # calculate mean, variance of each feature for each class\n",
    "        self.parameters = []  # {\"mean\": mean, \"var\": variance} of each feature (column), shape=(num_features, num_classes)\n",
    "        \n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            # only select rows where labels equal the given class (cls)\n",
    "            X_where_cls = X[np.where(y==cls)]\n",
    "            self.parameters.append([])\n",
    "            # add the mean and variance for each feature (column)\n",
    "            for col in X_where_cls.T:\n",
    "                parameters = {\"mean\": col.mean(), \"var\": col.var()}\n",
    "                self.parameters[idx].append(parameters)\n",
    "                \n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict the class labels of samples in X\n",
    "        '''\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def _calculate_prior(self, cls):\n",
    "        '''\n",
    "        Calculate prior (frequency) of class cls\n",
    "        prior = (samples where class == cls) / (total number of samples)\n",
    "        '''\n",
    "        prior = np.mean(self.y==cls)\n",
    "        return prior\n",
    "    \n",
    "    def _calculate_likelihood(self, x, mean, var):\n",
    "        '''\n",
    "        Calculate Gaussian likelhood of the data x given mean and variance\n",
    "        '''\n",
    "        eps = 1e-4  # added in denominator to prevent divison by zero\n",
    "        numerator = math.exp(-(math.pow(x-mean, 2) / (2 * var + eps)))\n",
    "        denominator = 1.0 / math.sqrt(2.0 * math.pi * var + eps) \n",
    "        return numerator * denominator\n",
    "        \n",
    "    \n",
    "    def _predict(self, x):\n",
    "        ''' Classification using Bayes Rule P(Y|X) = P(X|Y)*P(Y)/P(X),\n",
    "            or Posterior = Likelihood * Prior / Scaling Factor\n",
    "        P(Y|X) - The posterior is the probability that sample x is of class y given the\n",
    "                 feature values of x being distributed according to distribution of y and the prior.\n",
    "        P(X|Y) - Likelihood of data X given class distribution Y.\n",
    "                 Gaussian distribution (given by _calculate_likelihood)\n",
    "        P(Y)   - Prior (given by _calculate_prior)\n",
    "        P(X)   - Scales the posterior to make it a proper probability distribution.\n",
    "                 This term is ignored in this implementation since it doesn't affect\n",
    "                 which class distribution the sample is most likely to belong to.\n",
    "        Classifies the sample as the class that results in the largest P(Y|X) (posterior)\n",
    "        '''\n",
    "        \n",
    "        posteriors = []\n",
    "        # go through list of class\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            prior = self._calculate_prior(cls)\n",
    "            # initialize posterior for later calculation\n",
    "            posterior = 1\n",
    "            # calculate posterior for each class\n",
    "            for feature_val, params in zip(x, self.parameters[idx]):\n",
    "                likelihood = self._calculate_likelihood(feature_val, params[\"mean\"], params[\"var\"])\n",
    "                posterior *= prior * likelihood\n",
    "            posteriors.append(posterior)\n",
    "        # return the class with the largest posterior probability\n",
    "        return self.classes[np.argmax(posteriors)]                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d7f8173",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classification accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    "# testing model accuracy using synthetic data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "# generate synthetic data\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# test model's performance\n",
    "model = NaiveBayes()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c884936",
   "metadata": {},
   "source": [
    "### Naive Bayes implementation using sklearn\n",
    "\n",
    "NOTE: Following section implements Guassian Naive Bayes algorithm using `GaussianNB` module from `sklearn.naive_bayes` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "844b7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# other popular Naive Bayes modules: MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd5a0513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classification accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    "# testing model accuracy using synthetic data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "# generate synthetic data\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# test model's performance\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a06925",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- <a href=\"https://scikit-learn.org/stable/modules/naive_bayes.html\">Naive Bayes - sklearn </a>\n",
    "- <a href=\"https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/naive_bayes.py\">Machine Learning from Scratch - Naive Bayes </a>\n",
    "- <a href=\"https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/blob/main/06%20NaiveBayes/naive_bayes.py\">How to implement Naive Bayes from scratch with Python</a>\n",
    "- <a href=\"https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c\">Naive Bayes Classifier - towardsdatascience blog </a>\n",
    "- <a href=\"https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf\">All about Naive Bayes - towardsdatascience blog</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
