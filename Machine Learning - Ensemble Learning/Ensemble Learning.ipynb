{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891a18c4",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "**Ensemble Learning** is a general meta approach to machine learning that seeks better predictive performance by combining the predictions from multiple models <br><br>\n",
    "    \n",
    "Ensemble models is a machine learning approach to combine multiple other models in the prediction process. These weak models are referred to as **base estimators**.<br>\n",
    "It is a solution to overcome the following technical challenges of building a single estimator:\n",
    "\n",
    "- High variance: The model is very sensitive to the provided inputs to the learned features\n",
    "- Low accuracy: One model or one algorithm to fit the entire training data might not be good enough to meet expectations\n",
    "- Features noise and bias: The model relies heavily on one or a few features while making a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d5199",
   "metadata": {},
   "source": [
    "### Ensemble Algorithm\n",
    "\n",
    "Machine Learning algorithms have their limitations and producing a model with high accuracy and generalization is challenging. <br>\n",
    "A single algorithm may not make the perfect prediction for a given dataset, but if we build and **combine the learning of multiple models**, the overall accuracy and generalization could get boosted. <br>\n",
    "The combination can be implemented by aggregating the output from each model with two objectives: reducing the model error and maintaining its generalization.<br>\n",
    "These implementation techniques are often referred to as <u>meta-algorithms</u> <br>\n",
    "\n",
    "![ensembleLearning.png](images/ensembleLearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0d0bc",
   "metadata": {},
   "source": [
    "## Aggregation Techniques\n",
    "\n",
    "When we ensemble multiple algorithms to adapt the prediction process to combine multiple models, we need an aggregating method. Three main techniques can be used:\n",
    "\n",
    "### Max Voting\n",
    "The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction. <br>\n",
    "For example:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th> </th>\n",
    "            <th scope=\"col\">Judge 1 </th>\n",
    "            <th scope=\"col\">Judge 2 </th>\n",
    "            <th scope=\"col\">Judge 3 </th>\n",
    "            <th scope=\"col\">Judge 4 </th>\n",
    "            <th scope=\"col\">Judge 5 </th>\n",
    "            <th scope=\"col\">Final score </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th scope=\"row\">score </th>\n",
    "            <td>5 </td>\n",
    "            <td>4 </td>\n",
    "            <td>4 </td>\n",
    "            <td>4 </td>\n",
    "            <td>5 </td>\n",
    "            <th scope=\"row\">4 </th>\n",
    "        </tr>\n",
    "    </tbody>   \n",
    "</table>\n",
    "\n",
    "### Averaging\n",
    "Averaging is typically used for regression problems where base estimators' predictions are averaged to make the final prediciton.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th> </th>\n",
    "            <th scope=\"col\">Judge 1 </th>\n",
    "            <th scope=\"col\">Judge 2 </th>\n",
    "            <th scope=\"col\">Judge 3 </th>\n",
    "            <th scope=\"col\">Judge 4 </th>\n",
    "            <th scope=\"col\">Judge 5 </th>\n",
    "            <th scope=\"col\">Final score </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th scope=\"row\">score </th>\n",
    "            <td>5 </td>\n",
    "            <td>4.2 </td>\n",
    "            <td>4.6 </td>\n",
    "            <td>4 </td>\n",
    "            <td>5 </td>\n",
    "            <th scope=\"row\">4.36 </th>\n",
    "        </tr>\n",
    "    </tbody>   \n",
    "</table>\n",
    "\n",
    "### Weighted Average\n",
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. <br>\n",
    "For example: to ensemble scores given by professional judges and sports bloggers, we can assign them different weights\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th> </th>\n",
    "            <th scope=\"col\">Judge 1 </th>\n",
    "            <th scope=\"col\">Judge 2 </th>\n",
    "            <th scope=\"col\">Judge 3 </th>\n",
    "            <th scope=\"col\">Judge 4 </th>\n",
    "            <th scope=\"col\">Judge 5 </th>\n",
    "            <th scope=\"col\">Final score </th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th scope=\"row\">weight </th>\n",
    "            <td>0.23 </td>\n",
    "            <td>0.23 </td>\n",
    "            <td>0.18 </td>\n",
    "            <td>0.18 </td>\n",
    "            <td>0.18 </td>\n",
    "            <td></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th scope=\"row\">score </th>\n",
    "            <td>5 </td>\n",
    "            <td>4.2 </td>\n",
    "            <td>4.6 </td>\n",
    "            <td>4 </td>\n",
    "            <td>5 </td>\n",
    "            <th scope=\"row\">4.564 </th>\n",
    "        </tr>\n",
    "    </tbody>   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa5291",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d80c9",
   "metadata": {},
   "source": [
    "## Ensemble Techniques\n",
    "\n",
    "### Bagging\n",
    "\n",
    "The idea behing Bagging is combining the results of multiple models (for instance, Decision trees) trained on varying sub-datasets of the same dataset to get a generalized result. <br>\n",
    "**Bagging** is a combination of **Bootstrapping** and **Aggregation** <br>\n",
    "**Bootstrapping** --> **Random sampling with replacement**<br>\n",
    "**Boosting** --> **Combining results of multiple models**<br><br>\n",
    "    \n",
    "In other words, Bagging is training a bunch of individual models parallely, where each model is trained on a random subset of data with resampling (each subset is of same size) and combining the output or prediction of all these (weak) models via **voting for classification** or **average for regression**\n",
    "\n",
    "![bagging.png](images/bagging.png)\n",
    "\n",
    "- Multiple subsets are created from the original dataset, selecting observations with replacement\n",
    "- A base model (weak model) is created on each of these subsets\n",
    "- The models run in parallel and are independent of each other\n",
    "- The final predictions are determined by combining the predictions from all the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d72fc",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "**Boosting** is training a bunch of individual models in a **sequential** way. Each individual model learns from mistaked made by previous model. The succeeding models are dependent on the previous model.\n",
    "\n",
    "![boosting.png](images/boosting.png)\n",
    "\n",
    "- A subset is created from the original dataset. Initially, all data points are given equal weights\n",
    "- A base model is created on this subset. This model is used to make predictions on the whole dataset\n",
    "- Errors are calculated using the actual values and predicted values\n",
    "- The observations which are incorrectly predicted, are given higher weights. \n",
    "- Another model is created and predictions are made on the dataset. This model tries to correct the errors from the previous model\n",
    "- Similarly, multiple models are created, each correcting the errors of the previous model\n",
    "- The final model (strong learner) is the weighted mean of all the models (weak learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909d896",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "Stacking is similar to boosting models; they produce more robust predictors. Stacking is a process of learning how to create such a stronger model from all weak learners’ predictions.<br>\n",
    "In stacking, an algorithm takes the outputs of sub-models as input and attempts to learn how to best combine the input predictions to make a better output prediction. This works as a series of <br>\n",
    "**Original data** --> **Base models** --> **Level 0 predictions** which will now act as features to build new model --> **Meta model** --> **Level 1 prediction** and so on \n",
    "\n",
    "![stacking.png](images/stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d94842e",
   "metadata": {},
   "source": [
    "### Blending\n",
    "\n",
    "Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7422f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c596e966",
   "metadata": {},
   "source": [
    "## Algorithms based on Bagging and Boosting \n",
    "\n",
    "Bagging and Boosting are two of the most commomly used techniques in Machine Learning. These techniques inspired the follwoing widely used algorithms:\n",
    "\n",
    "1. Bagging algorithms:\n",
    "- Bagging meta-estimator\n",
    "- Random forest\n",
    "2. Boosting algorithms:\n",
    "- AdaBoost\n",
    "- GBM\n",
    "- XGBM\n",
    "- Light GBM\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344d00b",
   "metadata": {},
   "source": [
    "following are code snippets as to how to train Ensemble models using scikit-learn <br><br>\n",
    "**NOTE:** snippets use `X_train`, `y_train`, `X_test` and `y_test` which are training and test sets repectively i.e. it skips over importing libraries and data, Feature Engineerng, Exploratory Data Analysis, Data Analysis and Visualization and directly jumps to model building and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155be03b",
   "metadata": {},
   "source": [
    "## 1. Bagging Algorithms using scikit-learn\n",
    "\n",
    "### 1.1 Bagging meta-estimator\n",
    "Bagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. <br>\n",
    "Following are the steps for the bagging meta-estimator algorithm:\n",
    "1. Random subsets are created from the original dataset (Bootstrapping)\n",
    "2. The subset of the dataset includes all features\n",
    "3. A user-specified base estimator is fitted on each of these smaller sets\n",
    "4. Predictions from each model are combined to get the final result\n",
    "<br>\n",
    "\n",
    "Parameters for this algorithm in scikit-learn:\n",
    "- `base_estimator`: It defines the base estimator to fit on random subsets of the dataset. Default base estimator is Decision Tree\n",
    "- `n_estimators`: It is the number of base estimators to be created\n",
    "- `max_samples`: It is the maximum number of samples to train each base estimator\n",
    "- `max_features`: It defines the maximum number of features required to train each base estimator\n",
    "- `n_jobs`: The number of jobs to run in parallel. Set this value equal to the cores in your system by setting n_jobs=-1\n",
    "- `random_state`: It specifies the method of random split. This parameter is useful when you want to compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f609a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Classification\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test,y_test)\n",
    "print('Bagging meta-estimator accuracy :', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Regression\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test,y_test)\n",
    "print('Bagging meta-estimator accuracy :', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667af9c",
   "metadata": {},
   "source": [
    "### 1.2 Random Forest\n",
    "Random Forest uses radom sampling with replacement for Bagging and Decision Tree as base estimator. Random Forest selects a set of features which are used to decide the best split at each node of Decision Tree. It does so by measuring and comparing the Information Gain using each feature when used to split the tree at decision nodes using a function such as Gini Index or log loss.<br>\n",
    "Following are the steps for Random Forest algorithm:\n",
    "1. Random subsets are created from the original dataset (bootstrapping)\n",
    "2. At each node in the decision tree, only a random set of features are considered to decide the best split using Information Gain or Gini Index\n",
    "3. A decision tree model is fitted on each of the subsets\n",
    "4. The final prediction is calculated by aggregating the predictions from all decision trees\n",
    "\n",
    "Parameters for this algorithm in scikit-learn:\n",
    "- `n_estimators`: It defines the number of decision trees to be created in a random forest\n",
    "- `criterion`: It defines the function that is to be used for splitting. The function measures the quality of a split for each feature and chooses the best split\n",
    "- `max_features`: It defines the maximum number of features allowed for the split in each decision tree\n",
    "- `max_depth`: Random forest has multiple decision trees. This parameter defines the maximum depth of the trees\n",
    "- `min_samples_split`: Used to define the minimum number of samples required in a leaf node before a split is attempted. If the number of samples is less than the required number, the node is not split\n",
    "- `min_samples_leaf`: This defines the minimum number of samples required to be at a leaf node\n",
    "- `max_leaf_nodes`: This parameter specifies the maximum number of leaf nodes for each tree. The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node\n",
    "- `n_jobs`: This indicates the number of jobs to run in parallel. Set value to -1 if you want it to run on all cores in the system.\n",
    "- `random_state`: This parameter is used to define the random selection. It is used for comparison between various models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a38cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Classification\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "score = model.score(X_test,y_test)\n",
    "print('Bagging meta-estimator accuracy :', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Regression\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "score = model.score(X_test,y_test)\n",
    "print('Bagging meta-estimator accuracy :', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c2186",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046345f",
   "metadata": {},
   "source": [
    "## 2. Libraries and Frameworks for Boosting Algorithms \n",
    "\n",
    "### 2.1 AdaBoost : scikit-learn\n",
    "Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Multiple sequential models are created, each correcting the errors from the last model. Usually, decision trees are used for modelling.  AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly by adjusting the weights of incorrectly predicted observations (data points). <br>\n",
    "\n",
    "Following are the steps for AdaBoost algorithm:\n",
    "\n",
    "1. Initially, all observations in the dataset are given equal weights\n",
    "2. A model is built on a subset of data, usually a Decision Tree\n",
    "3. Using this model, predictions are made on the whole dataset\n",
    "4. Errors are calculated by comparing the predictions and actual values\n",
    "5. While creating the next model, higher weights are given to the data points which were predicted incorrectly. Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation\n",
    "6. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached\n",
    "\n",
    "Parameters for this algorithm in scikit-learn:\n",
    "\n",
    "- `base_estimators`: specifies the machine learning algorithm to be used as base learner\n",
    "- `n_estimators`: defines the number of base estimators. Defaullt values is 10\n",
    "- `learning_rate`: it controls the contribution of the estimators in the final combination. \n",
    "There is a trade-off between learning_rate and n_estimators.\n",
    "- `max_depth`: defines the maximum depth of the individual estimator\n",
    "- `n_jobs`: specifies the number of processors it is allowed to use. Set value to -1 for maximum processors allowed\n",
    "- `random_state` : an integer value to specify the random data split. A definite value of random_state will always produce same results if given with same parameters and training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df372ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Classification\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaBoost = AdaBoostClassifier(random_state=1)\n",
    "adaBoost.fit(X_train, y_train)\n",
    "score = adaBoost.score(X_test,y_test)\n",
    "print('Adaboost model accuracy :', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37657848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Regression\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "adaBoost = AdaBoostRegressor()\n",
    "adaBoost.fit(X_train, y_train)\n",
    "score = adaBoost.score(X_test,y_test)\n",
    "print('Adaboost model accuracy :', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce429f",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Boosting (GBM) : scikit-learn\n",
    "Gradient Boosting Model uses an internal regression model trained iteratively on the residuals, combining a number of weak learners to form a strong learner. Each subsequent tree in series is built on the errors calculated by the previous tree. The error produced by a tree is combined with the input of that tree to form the input of next subsequent tree i.e. **Input(tree 1) + Error(tree 1) = Output(tree 1) = Input(tree 2)** and so on <br>\n",
    "\n",
    "Following are the steps for Gradient Boosting algorithm:\n",
    "1. Train a weak learner-model 1 using given data-input 1 and predict the required output using this model-prediction 1\n",
    "2. Calculate the error(residual) of that model-error 1\n",
    "3. Combine the input of previous model (input 1) with the calculated residual using a learning rate. This will be the input for next iteration of model $output 1 = input 2 = input 1 + \\alpha*error 1$\n",
    "3. Iteratively create a new model-model 2 using the error calculated as target variable. The objective is to find the best split to minimize the error\n",
    "4. Predictions made by this model (model 1) are combined with predictions made by previous model (model 2). This value is the new prediction-prediction 2\n",
    "5. New errors are created using this prediction prediction 2\n",
    "6. Steps 2 to 5 are repeated till the maximum number of iterations is reached (or error function does not change)\n",
    "\n",
    "Parameters for this algorithm in scikit-learn:\n",
    "\n",
    "- `min_samples_split`: defines the minimum number of samples (or observations) which are required in a node to be considered for splitting. It is used to control over-fitting\n",
    "- `min_samples_leaf`: defines the minimum samples required in a terminal or leaf node\n",
    "- `min_weight_fraction_leaf`: similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer\n",
    "- `max_depth`: maximum depth of a tree. Used to control over-fitting. Should be tuned using CV\n",
    "- `max_leaf_nodes`: maximum number of terminal nodes or leaves in a tree. If this is defined, GBM will ignore max_depth\n",
    "- `max_features`: number of features to consider while searching for the best split. These will be randomly selected. As a thumb-rule, the square root of the total number of features works great but we should check up to 30-40% of the total number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e905e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Classification\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gradBoost = GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "gradBoost.fit(X_train, y_train)\n",
    "score = gradBoost.score(X_test,y_test)\n",
    "print('Gradient Boosting Model accuracy :', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75745d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Regression\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gradBoost = GradientBoostingRegressor()\n",
    "gradBoost.fit(X_train, y_train)\n",
    "score = gradBoost.score(X_test,y_test)\n",
    "print('Gradient Boosting Model accuracy :', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f4923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e61139b5",
   "metadata": {},
   "source": [
    "### 2.3 XGBoost (XGB)\n",
    "XGBoost stands for **eXtreme Gradient Boosting** and it’s an open-source implementation of the gradient boosted trees algorithm. <br>\n",
    "XGBoost algorithm uses Gradient Boosting framework and uses **Gradient Descent** to optimize the loss function.<br>\n",
    "It's implementation was specifically engineered for optimal performance and speed. <br><br>\n",
    "\n",
    "In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured/tabular data, decision tree based algorithms are considered best-in-class right now. <br><br>\n",
    "\n",
    "#### Hyperparameters\n",
    "- `gamma`: helps with controlling overfitting. It specifies the minimum reduction in the loss required to make a further partition on a leaf node of the tree\n",
    "- `booster`\n",
    "- `reg_alpha` and `reg_lambda`\n",
    "- `max_depth`\n",
    "- `subsample`\n",
    "- `num_estimators`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6082dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, using it with library, XGBoost all the time?, further exploration(sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4dcd8",
   "metadata": {},
   "source": [
    "In order for XGBoost to be able to use our data, we’ll need to transform it into a specific format that XGBoost can handle. That format is called DMatrix. It’s a very simple one-linear to transform a numpy array of data to DMatrix format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c39cf",
   "metadata": {},
   "source": [
    "Setting the optimal hyperparameters of any ML model can be a challenge. So why not let Scikit Learn do it for you? We can combine Scikit Learn’s grid search with an XGBoost classifier quite easily:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e753609",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "parameters = {\n",
    "     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "     }\n",
    "\n",
    "grid = GridSearchCV(clf,\n",
    "                    parameters, n_jobs=4,\n",
    "                    scoring=\"neg_log_loss\",\n",
    "                    cv=3)\n",
    "\n",
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f03e7c",
   "metadata": {},
   "source": [
    "Only do that on a big dataset if you have time to kill — doing a grid search is essentially training an ensemble of decision trees many times over!<br><br>\n",
    "\n",
    "Once your XGBoost model is trained, you can dump a human readable description of it into a text file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c438d33",
   "metadata": {},
   "source": [
    "model.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164cb08",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1a5aa",
   "metadata": {},
   "source": [
    "### 2.4 LightGBM\n",
    "LightGBM, short for Light Gradient Boosting Machine, is a free and open source distributed gradient boosting framework for machine learning originally developed by Microsoft. <br>\n",
    "Light GBM beats all the other algorithms **when the dataset is extremely large**. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset. <br>\n",
    "LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern. The images below will help you understand the difference in a better way.\n",
    "\n",
    "![lightGBM.png](images/lightGBM.png)\n",
    "\n",
    "Leaf-wise growth may cause over-fitting on smaller datasets but that can be avoided by using the ‘max_depth’ parameter for learning. <br>\n",
    "\n",
    "Parameters for this algorithm in scikit-learn: \n",
    "\n",
    "- `num_iterations`: defines the number of boosting iterations to be performed\n",
    "- `num_leaves` : set the number of leaves to be formed in a tree. In case of Light GBM, since splitting takes place leaf-wise rather than depth-wise, num_leaves must be smaller than 2^(max_depth), otherwise, it may lead to overfitting\n",
    "- `min_data_in_leaf`: minimum number of data/sample/count per leaf (default is 20. A very small value may cause overfitting\n",
    "- `max_depth`: specifies the maximum depth or level up to which a tree can grow\n",
    "- `bagging_fraction`: specify the fraction of data to be used for each iteration\n",
    "- `max_bin`: defines the max number of bins that feature values will be bucketed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ffc2a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-1ccf7ef61d85>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-1ccf7ef61d85>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    y_pred[i]=1\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# for Classification\n",
    "\n",
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "#define parameters\n",
    "params = {'learning_rate':0.001}\n",
    "model= lgb.train(params, train_data, 100) \n",
    "y_pred=model.predict(X_test)\n",
    "for i in range(0, 185):\n",
    "    if y_pred[i]>=0.5: \n",
    "    y_pred[i]=1\n",
    "else: \n",
    "    y_pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a459997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Regression\n",
    "\n",
    "import lightgbm as lgb\n",
    "train_data=lgb.Dataset(X_train, label=y_train)\n",
    "params = {'learning_rate':0.001}\n",
    "model= lgb.train(params, train_data, 100)\n",
    "y_pred = mode.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(y_pred, y_test)**0.5\n",
    "print('Root Mean Square Error :', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcfbcc",
   "metadata": {},
   "source": [
    "### 2.5 CatBoost\n",
    "CatBoost is an open-source software library developed by Yandex. It provides a gradient boosting framework which among other features attempts to solve for Categorical features using a permutation driven alternative compared to the classical algorithm.<br>\n",
    "CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms. This reduces the computations required drastically when categorical variables have too many labels as performing one-hot-encoding on them exponentially increases the dimensionality of the dataset.<br><br>\n",
    "CatBoost algorithm effectively deals with categorical variables. Thus, you should not perform one-hot encoding for categorical variables. Just load the files, impute missing values, and you’re good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30035f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Classification\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "catBoost = CatBoostClassifier()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "catBoost.fit(X_train, y_train, cat_features=([0, 1, 2, 3, 4, 10]), eval_set=(X_test, y_test))\n",
    "score = catBoost.score(x_test,y_test)\n",
    "print('Catboost Model accuracy :', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc37ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Regression\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "catBoost = CatBoostRegressor()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "catBoost.fit(X_train, y_train, cat_features=([ 0,  1, 2, 3, 4, 10]), eval_set=(X_test, y_test))\n",
    "score = scorecatBoost.score(x_test,y_test)\n",
    "print('Catboost Model accuracy :', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f793e4a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4160bb6f",
   "metadata": {},
   "source": [
    "## Analogy for evolution of tree-based algorithms\n",
    "\n",
    "1. **Decision Tree:** Every hiring manager has a set of criteria such as education level, number of years of experience, interview performance. A decision tree is analogous to a hiring manager interviewing candidates based on his or her own criteria\n",
    "2. **Bagging:** Now imagine instead of a single interviewer, now there is an interview panel where each interviewer has a vote. Bagging or bootstrap aggregating involves combining inputs from all interviewers for the final decision through a democratic voting process\n",
    "3. **Random Forest:** It is a bagging-based algorithm with a key difference wherein only a subset of features is selected at random. In other words, every interviewer will only test the interviewee on certain randomly selected qualifications (e.g. a technical interview for testing programming skills and a behavioral interview for evaluating non-technical skills)\n",
    "4. **Boosting:** This is an alternative approach where each interviewer alters the evaluation criteria based on feedback from the previous interviewer. This ‘boosts’ the efficiency of the interview process by deploying a more dynamic evaluation process\n",
    "5. **Gradient Boosting:** A special case of boosting where errors are minimized by gradient descent algorithm e.g. the strategy consulting firms leverage by using case interviews to weed out less qualified candidates\n",
    "6. **XGBoost:** Think of XGBoost as gradient boosting on ‘steroids’ (well it is called ‘Extreme Gradient Boosting’ for a reason!). It is a perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ecb27",
   "metadata": {},
   "source": [
    "## Careful Considerations\n",
    "\n",
    "- **Noise, bias, and Variance:** :The combination of decisions from multiple models can help improve the overall performance. Hence, one of the key factors to use ensemble models is overcoming these issues: noise, bias, and variance. If the ensemble model does not give the collective experience to improve upon the accuracy in such a situation, then a careful rethinking of such employment is necessary\n",
    "\n",
    "- **Simplicity and Explainability:** Machine learning models are preferred to be simpler than complicated. The ability to explain the final model decision is empirically reduced with an ensemble\n",
    "\n",
    "- **Generalizations:** Ensemlble models with no careful training process can quickly produce overfitting models\n",
    "\n",
    "- **Inference Time:** Inference Time i.e the time required by a model to generate a prediction from a trained model with new/live data points is criticial in a Machine Learning environment. When deploying ensemble models into production, the amount of time needed to pass multiple models increases and could increase the inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8630870e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Ensemble models is an excellent method for machine learning. The ensemble models have a variety of techniques for classification and regression problems and are able to reduce bias at the cost of possible overfitting and less generalised model. <br>\n",
    "It is recommended to **create various Machine Learning models using different Ensemble techniques and compare each model's performance**(using tabular comparison or heatmap) to finally select the best fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4ac93",
   "metadata": {},
   "source": [
    "### Tree based models vs Neural Networks \n",
    "\n",
    "Neural Networks tend to work tremendously well with unorganized data such as image, text or audio files, but tree-based models (XGBoost & random forests) outperform Neural Networks (NNs) on tabular datasets. <br>\n",
    "Tree-based models offer more accurate predictions with less computation cost when the dataset size is small/medium (~10k data samples). <br>\n",
    "Also, in such a setting, they perform well even when there is: \n",
    "- irregular pattern in the target function\n",
    "- uninformative features\n",
    "- non-rotationally invariant data <br>\n",
    "\n",
    "However, this might not hold when additional regularization techniques such as Data Augmentation are added to random search or when the dataset size is massive <br>\n",
    "#### So should we use just XGBoost all the time? \n",
    "We must test all possible algorithms for data at hand to identify the champion algorithm. Besides, picking the right algorithm is not enough. We must also choose the right configuration of the algorithm for a dataset by tuning the hyper-parameters. Furthermore, there are several other considerations for choosing the winning algorithm such as computational complexity, explainability, and ease of implementation. This is exactly the point where Machine Learning starts drifting away from science towards art, but honestly, that’s where the magic happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2864ff7",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- <a href=\"https://towardsdatascience.com/ensemble-models-5a62d4f4cb0c\">Ensemble Models - towardsdatascience </a>\n",
    "- <a href=\"https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\">A Comprehensive Guide to Ensemble Learning - analyticsvidhya</a>\n",
    "- <a href=\"https://www.javatpoint.com/stacking-in-machine-learning#:~:text=Stacking%20is%20one%20of%20the,new%20model%20with%20improved%20performance.\">Stacking in Machine Learning - javaTpoint </a>\n",
    "- <a href=\"https://hal.archives-ouvertes.fr/hal-03723551\">Why do tree-based models still outperform deep learning on tabular data? </a>\n",
    "- <a href=\"https://github.com/manikpurivibhu/Python-Machine-Learning-notes/blob/master/Machine%20Learning%20-%20Random%20Forest/Random%20Forest.ipynb\">Random Forest - manikpurivibhu </a>\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Gradient_boosting#Algorithm\"> Gradient Boosting - Wikipedia  </a>\n",
    "- <a href=\"https://towardsdatascience.com/xgboost-theory-and-practice-fb8912930ad6\">  XGBoost: theory and practice - towardsdatascience   </a>\n",
    "- <a href=\"https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d\">XGBoost Algorithm: Long May She Reign! - towardsdatascience   </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
