{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d1118f",
   "metadata": {},
   "source": [
    "# Linear Regression Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd52ac",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd19da",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077528ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import statsmodels.stats.multicomp\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58c81a5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106297c4",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n",
    "load dataset from csv, xlsx or other formats into variable (say, df) using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('filename.csv')\n",
    "#OR\n",
    "df = pd.read_csv(https://url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd76e1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cf8e6",
   "metadata": {},
   "source": [
    "## clean data\n",
    "\n",
    "Deal with empty values in the dataset. These may be _NaN_ values, ',' or anything.<br>\n",
    "Find out what these are (if any) using pandas.head()/ pandas.tail()/ pandas.sample() and deal with them by either dropping the rows which have these faulty datasets or replacing them with some other value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe53f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "##OR\n",
    "\n",
    "df['some_imp_col'].replace(np.nan, 0)\n",
    "df['imp_col'].replace(np.nan, np.mean(df['imp_col']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51683f0e",
   "metadata": {},
   "source": [
    "#### change dtypes of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af778d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['col1', 'col_n']] = df[['col1', 'col_n']].astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ea58e",
   "metadata": {},
   "source": [
    "#### rearranging the dataset columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccb43f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = df.columns.to_list()\n",
    "l = len(d)\n",
    "d[l-2], d[l-1] = d[l-1], d[l-2]\n",
    "df = df[d]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bffbb4",
   "metadata": {},
   "source": [
    "#### cleaning column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faaea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fffb28",
   "metadata": {},
   "source": [
    "#### Dealing with copy variables with case sensitivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.columns)-1):\n",
    "    if df.dtypes[i] == 'object':\n",
    "        df[datdfaset.columns[i]] = dataset[dataset.columns[i]].str.lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea045ad0",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182c584",
   "metadata": {},
   "source": [
    "## Data Standardzation\n",
    "\n",
    "Data standardization is the process of rescaling the attributes so that they have mean as 0 and variance as 1. <br>\n",
    "This refers to bringing down all the features to a common scale without distorting the differences in the range of the values.\n",
    "ex: np.arange(0.01,1,0.01), np.arange(1, 10000, 10)] to [np.arange(0.01,1,0.01), np.arange(0.01, 1, 0.01)]\n",
    "<br><br>\n",
    "For this, each data point in  the col = (data point - mean of col)/std of col<br><br>\n",
    "Though this is done easily using `sklearn.preprocessing.StandardScaler` , some additional context to the data is also required<br>\n",
    "ex: if col8: mileage(kmph) in city and col9: mileage(mpg) in highway, then first convert either mpg to kmph or vice versa and then StandardScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "standard_df = scaler.fit_transform(df)\n",
    "df = pd.DataFrame(standard_df, columns =['col_1', 'col_n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198ac66",
   "metadata": {},
   "source": [
    "There are also some Data Standardization techniques other than StandardScaler()<br><br>\n",
    "<b>StandardScaler</b> follows Standard Normal Distribution (SND). Therefore, it makes mean = 0 and scales the data to unit variance.<br><br>\n",
    "<b>MinMaxScaler</b> scales all the data features in the range [0, 1] or else in the range [-1, 1] if there are negative values in the dataset.<br><br>\n",
    "By using <b>RobustScaler()</b>, we can remove the outliers and then use either StandardScaler or MinMaxScaler for preprocessing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58595e43",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed626388",
   "metadata": {},
   "source": [
    "## Bin data\n",
    "\n",
    "Binning is the process of groupng values together into 'bins' <br>\n",
    "ex: 'bin; age into 'kid', 'teen,', 'adult', 'middle-aged', etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(df['col_to_be_binned']), max(df['col_to_be_binned']), 4)\n",
    "#this divides the 'col_to_be_binned' into 4 bins w.r.t their min and max val\n",
    "\n",
    "bin_names = ['bin1_small', 'bin2_medium', 'bin3_large']\n",
    "#i.e. a list of bin names\n",
    "\n",
    "df['binned_col'] = pd.cut(df['col_to_be_binned'], bins, label = bin_names, include_lowest = True)\n",
    "\n",
    "#this creates a new col in the DataFrame with binned data\n",
    "#you can delete the  original column which was binned to reduce dataframe size\n",
    "\n",
    "df.drop(['col_to_be_binned'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09608927",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc837846",
   "metadata": {},
   "source": [
    "## Categorical Variables into Numerical Variables\n",
    "\n",
    "Categorical Variables (ex: df['sex'].unique() = ['M', 'F'] needs to be converted into Numeircal Variables to be included in the ML algorithm<br><br>\n",
    "\n",
    "This can be achieved by:\n",
    "\n",
    "1. <b>Dummy Variables</b> :  A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "In our example, it will create 2 columns and provide each row w with [0, 1] if data is Male and [1, 0] if data is Female.<br>\n",
    "Dummy Variable works well while you are exploring and analyzing but for final inclusion in the dataset, OneHotEncoding is much more suitable.<br><br>\n",
    "\n",
    "2. <b>One Hot Encoding</b> : It works essentially the same as Dummy Vairables the only difference is that it excludes 1 row. This works with the concept that the variables have a linear relationship and hence for n unique categories in categorical variable, there are n-1 new vairables. \n",
    "In our example, it will create 1 column and provide each row w with [0] if data is Male and [1] if data is Female.<br><br>\n",
    "\n",
    "3. <b>Label Encoding</b> : It follows Ordinal Scaling i.e. for each category it establishes a relationship among the categories as per their ranking.<br>\n",
    "ex: for categories : ['low', 'medium', 'high'], it will be twice and thrice as affective for 'medium' and 'high' respectively than for 'low'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35973d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dummy Vairables\n",
    "\n",
    "dvar = pd.get_dummies(df['col_to_be_binned'], columns = ['cat_1', 'cat_n'])\n",
    "df = pd.concat([df, dvar], axis = 1)\n",
    "df.drop(df['col_to_be_binned'], axis=1, inplace=True)\n",
    "\n",
    "# you may drop 1 dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LabelEncoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()                               #labelencoderobject\n",
    "\n",
    "copy_df = df\n",
    "copy_df.cat_var = le.fit_transform(copy_df.cat_var)\n",
    "#this LeabelEncodes the 'cat_var' category of 'copy_df' DataFrame and puts it back in the 'cat_var' category of 'copy_df' DataFrame\n",
    "#i.e. all the categories in our categorical variables are converted into int numbers (0,1,2,..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OneHotEncoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()                             #onehotencoder object\n",
    "\n",
    "encoded_cat_col = ohe.fit_transform(copy_df.cat_var).toarray()\n",
    "#onehotencoded_col = ohe.fit_Transform(label_encoded_col)\n",
    "df = df.join(encoded_cat_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90167bee",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdaaa3",
   "metadata": {},
   "source": [
    "# Now we have Clean and preprocessed data and so, we should perform analysis on it and build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed8b94",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "It is the approach of analyzing data sets to summarize their main characteristics by\n",
    "\n",
    "1. Statistical Analysis\n",
    "2. Statistical Graphics\n",
    "3. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7ed31",
   "metadata": {},
   "source": [
    "![Data Statistical and Graphical Analysis](Images/EDA_cheat_sheet.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, 'unique values :', df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba075d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472c413",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def70ba",
   "metadata": {},
   "source": [
    "## Grouping Data\n",
    "\n",
    "Pandas `dataframe.groupby()` function is used to split the data into groups based on some criteria. <br>\n",
    "Pandas objects can be split on any of their axes. (row/col) <br>\n",
    "The abstract definition of grouping is to provide a mapping of labels to group names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_players = df.group_by(['Team', 'Position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_perfomance = df.group_by(['Team'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781d7c4",
   "metadata": {},
   "source": [
    "for grouping 3 variables, `pandas.pivot(index, columns, values)` function produces <b>pivot table</b> based on 3 columns of the DataFrame. Uses unique values from index / columns and fills with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ff6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ex:\n",
    "\n",
    "import pandas as pd\n",
    "  \n",
    "pivot_df = pd.DataFrame({'A': ['John', 'Cena', 'Mina'],\n",
    "      'B': ['Masters', 'Masters', 'Graduate'],\n",
    "      'C': [27, 23, 21]})\n",
    "  \n",
    "print(pivot_df, '\\n\\n')\n",
    "print(pivot_df.pivot('A', 'B', 'C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a6021",
   "metadata": {},
   "source": [
    "### Stastistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b2121",
   "metadata": {},
   "source": [
    "Statistical Association between variavbles:\n",
    "\n",
    "1. <b>Numerical-Numerical </b>: Pearson Correlation, Spearman Correlation<br><br>\n",
    "2. <b>Categorical - Categorical </b>: Chi-square Test <br><br>\n",
    "3. <b>Numerical - Categorical</b> : ANOVA Test(one-way, two-way, n-way). Below we are using one-way Anova<br>\n",
    "Analysis of Variance: checks the difference in mean for different categories against the same Numerical var. ex: is M/F mean for income same or different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07182d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df is dataset where last var is the target var\n",
    "target_var = df.iloc['target_col']\n",
    "\n",
    "for i in range(len(df.columns)-1):\n",
    "    \n",
    "    #for numerical-numerical\n",
    "    if df.dtypes[i] != 'object':\n",
    "        \n",
    "        pearson_coef, p_val = stats.pearsonr(df[df.columns[i]], df['target_col'])\n",
    "        print('Pearson Coefficient :',pearson_coef,'\\nP Value :',p_val,'\\n\\n')\n",
    "        \n",
    "        spearman_coef, p_val = stats.spearmanr(df[df.columns[i]], df['target_col'])\n",
    "        print('Spearman Coefficient :',spearman_coef,'\\nP Value :',p_val,'\\n\\n')\n",
    "        \n",
    "    #fot numerical-categorical    \n",
    "    else:\n",
    "        #one-way ANOVA\n",
    "        F, p = stats.f_oneway(df.columns[i],df['target_var'])\n",
    "        print('F-Statistic=%.3f, p=%.3f' % (F, p))\n",
    "\n",
    "        #two-way ANOVA\n",
    "        model = ols('{t} ~ {v}'.format(t = 'target_var', v = str(df.columns[i])df).fit()\n",
    "        print(f\"F-Statistic {model.fvalue: .3f}, \\nP-Value: {model.f_pvalue: .4f} \\n\\n\")\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Chi-Square Test\n",
    "\n",
    "#crosstable of categorical variables\n",
    "df_table = pd.crosstab(df['cat_1'], df['cat_n'])\n",
    "observed_values = df_Table.values\n",
    "print(observed_values)\n",
    "\n",
    "val = stats.chi2_contingency(df_table)\n",
    "print(val)\n",
    "\n",
    "expected_values = val[3]\n",
    "print(expected_values)\n",
    "\n",
    "ddof = len((df_table[0]-1)*(df_table[1]-1))\n",
    "print('Degree of Freedom :', ddof)\n",
    "\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2663fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "chi_square=sum([(o-e)**2./e for o,e in zip(observed_values,expected_values)])\n",
    "chi_square_statistic=chi_square[0]+chi_square[1]\n",
    "\n",
    "print(\"chi-square statistic:-\",chi_square_statistic)\n",
    "\n",
    "critical_value=chi2.ppf(q=1-alpha,df=ddof)\n",
    "print('critical_value:',critical_value)\n",
    "\n",
    "#p-value\n",
    "p_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\n",
    "print('p-value:',p_value)\n",
    "print('Significance level: ',alpha)\n",
    "print('Degree of Freedom: ',ddof)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b849874",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chi_square_statistic>=critical_value:\n",
    "    print(\"Reject H0,There is a relationship between 2 categorical variables\")\n",
    "else:\n",
    "    print(\"Retain H0,There is no relationship between 2 categorical variables\")\n",
    "    \n",
    "if p_value<=alpha:\n",
    "    print(\"Reject H0,There is a relationship between 2 categorical variables\")\n",
    "else:\n",
    "    print(\"Retain H0,There is no relationship between 2 categorical variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596e068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85bb2b52",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data.\n",
    "<br><br>\n",
    "Correlation coefficients are indicators of the strength of the linear relationship between two different variables, x and y. <br>\n",
    "A linear correlation coefficient that is greater than zero indicates a positive relationship. <br>\n",
    "A value that is less than zero signifies a negative relationship.<br>\n",
    "Finally, a value of zero indicates no relationship between the two variables x and y.<br>\n",
    "A correlation coefficient close to 1 or -1 indicatesa a strong positive or negative correlation between the variables<br><br>\n",
    "\n",
    "A good means to check correlation betwee 2 variables is \n",
    "1. scatter plot between variables and checking the slope of regression line\n",
    "2. calculating correlation coefficient mathematically \n",
    "\n",
    "Note: Correlation does not mean Causation\n",
    "\n",
    "`df.corr(method='pearson')` is used to find the pairwise correlation of all columns in the dataframe. <br>Method can be selected from {‘pearson’, ‘kendall’, ‘spearman’}. <br>\n",
    "Any na values are automatically excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28366656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1dc981",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283892a6",
   "metadata": {},
   "source": [
    "## Plot data\n",
    "\n",
    "Plot each variable against the target variable<br><br>\n",
    "\n",
    "If the variable is <b>Numerical</b>(and continuous), plot a <b>Scatterplot</b><br>\n",
    "If the variable is <b>Categorical</b>, plot a  <b>BoxPlot</b> and <b>Bar Chart</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class plot_data:\n",
    "    \n",
    "    def __init__(self, data, target):\n",
    "        self.dataset = data\n",
    "        self.target_var  = data[target_var]\n",
    "        \n",
    "        \n",
    "    def numerical(self, depen_var):\n",
    "        sns.scatterplot(x=self.dataset[depen_var], y=self.target_var)\n",
    "        plt.xlabel(depen_var.upper())\n",
    "        plt.ylabel(target_var.upper())\n",
    "        \n",
    "    def categorical(self, depen_var):\n",
    "        fig, axes = plt.subplots(1, 2, sharex=True, figsize=(12,8))\n",
    "        fig.suptitle('{c} vs {n}'.format(c = depen_var.upper(), n = target_var.upper()))\n",
    "        \n",
    "        if df[depen_var].nunique >= 6:\n",
    "            plt.xticks(rotation=90)\n",
    "            \n",
    "        sns.barplot(ax=axes[0], x = df[depen_var], y = df[target_var])\n",
    "        axes[0].set_title('Boxplot')\n",
    "\n",
    "        sns.barplot(ax=axes[1],  x = df[depen_var], y = df[target_var])\n",
    "        axes[1].set_title('Bar Chart')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = plot_data(df, 'target_variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through all the columns in dataframe, check if it is numerical or categorical and plot using the plot_data class methods\n",
    "\n",
    "for i in range(len(df.dtypes)):\n",
    "    \n",
    "    if df.columns[i] == 'some_specific_col(s)':\n",
    "        #some special operation or simply\n",
    "        pass\n",
    "        \n",
    "    elif df.columns[i] == target_var:\n",
    "        pass\n",
    "    \n",
    "    #plot categorical variables \n",
    "    elif df.dtype[i] == 'object':\n",
    "        instance.categorical(df.columns[i])\n",
    "        \n",
    "    #plot numerical variables\n",
    "    else:\n",
    "        instance.numerical(df.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd669a",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f012e1",
   "metadata": {},
   "source": [
    "# Now we have some understanding of the data and so we should proceed into model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c12cd",
   "metadata": {},
   "source": [
    "## train_test_split\n",
    "\n",
    "`sklearn.model_selection.train_test_split()` splits arrays or matrices into random train and test subsets<br><br>\n",
    "Use the training set to train the Machine Learning model and the test set to evaluate it's perfomance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96103c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df['target_var']\n",
    "x_data = df.drop(columns= ['target_var', \n",
    "                           'other_irrelevant_columns_as_shown_by_data/statistical_analysis'],\n",
    "                          inplace = True)\n",
    "\n",
    "##y_data is target data\n",
    "##x_data is parameters that affect the target data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92f11f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e25f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2e46a",
   "metadata": {},
   "source": [
    "## Model Buildinig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c74434b",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312f3e2",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "\n",
    "<br>\n",
    "\n",
    "`regression_obj = LinearRegression()                       #Regression Object` <br>\n",
    "`regression_obj.fit(indep_var_train, dep_var_train)        #Train Machine Learning Model`<br>\n",
    "`pred = regression_obj.predict(indep_var_test)             #Predict Values for new input`<br>\n",
    "`print('Intercept : ', regression_obj.inercept_ )          #Intercept of Linear Regression Model`<br>\n",
    "`print('Coefficient : ', regression_obj.coef_)             #Coefficient of Linear Regression Model`<br>\n",
    "`print('Some estimations : \\n', pred[:10])                 #some predictions of input values`<br>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression object\n",
    "slr = LinearRegression()\n",
    "\n",
    "slr_var = input('linear regression against which variable ?')\n",
    "\n",
    "#train model\n",
    "slr.fit(x_train[slr_var], y_train)\n",
    "\n",
    "pred = slr.predict(x_test[slr_var])\n",
    "\n",
    "#model score\n",
    "print('Score :', slr.score(x_test[slr_var], y_test))\n",
    "\n",
    "#model attributes\n",
    "print('Intercept : ', slr.inercept_ )\n",
    "print('Coefficient : ', slr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "##OR simply\n",
    "for col in x_train.columns:\n",
    "    slr.fit(x_train[col], y_train)\n",
    "\n",
    "    pred = slr.predict(x_test[col])\n",
    "\n",
    "    print(col.upper(), 'vs target_var Simple Linear Regression model perfomance')\n",
    "    \n",
    "    #model score\n",
    "    print('Score :', slr.score(x_test[col], y_test))\n",
    "    \n",
    "    #model attributes\n",
    "    print('Intercept : ', regression_obj.inercept_ )\n",
    "    print('Coefficient : ', regression_obj.coef_,'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c97933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print Simple Linear Regression Score for every parameter\n",
    "\n",
    "print('{} vs target_var Simple Linear Regression Score : slr_score[i]'.format(list(x_data.columns)[i]) for i in range(len(x_data.columns))):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7deea7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337856e5",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b080ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression object\n",
    "mlr = LinearRegression()\n",
    "\n",
    "#training the model\n",
    "mlr.fit(x_train, y_train)\n",
    "\n",
    "#predict values\n",
    "pred_mlr = mlr.predict(x_test)\n",
    "\n",
    "#model score\n",
    "print('Score :', slr.score(pred_mlr, y_test))\n",
    "\n",
    "#model attributes\n",
    "print('Intercept : ', mlr.inercept_ )\n",
    "print('Coefficient : ', mlr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ecd9c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13f995",
   "metadata": {},
   "source": [
    "### Polynomial Regression \n",
    "\n",
    "for curvlinear relationship between Dependent and Independent variables<br>\n",
    "\n",
    "<b>y = a + b1x + b2x^2 +....+ bnx^n</b>\n",
    "\n",
    "<br>\n",
    "\n",
    "`n = 2                                        #some int val`<br>\n",
    "`poly_reg = PolynomialFeatures(degree = n)    #polynomial regression (of order n) object`<br>\n",
    "`x_data_poly = poly_reg.fit_transform(x_data)`# convert your feature matrix into polynomial feature matrix, and then fitting it to the Polynomial regression model.<br><br>\n",
    "`lin_reg_2 = LinearRegression()               `#Linear Regression object that we will now fit with polynomial feature matrix<br>\n",
    "`lin_reg_2.fit(x_data_poly, y_data)           #Train Regression object with Polynomial Features`<br>\n",
    "`lin_reg_2.predict(y_data)                    #Predict results for input`<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e32796",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg = PolynomialFeatures(degree=2)\n",
    "x_poly_data = poly_reg.fit_transform(x_data)\n",
    "  \n",
    "print(x_poly_data)     # prints X_poly\n",
    " \n",
    "lin_reg2 = LinearRegression()\n",
    "lin_reg2.fit(x_poly_data,y_data)\n",
    " \n",
    "pred_poly = lin_reg2.predict(x_poly_data)\n",
    "print(pred_poly[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e115b",
   "metadata": {},
   "source": [
    "### Polynomial Regression for only 1 feature\n",
    "\n",
    "`f = np.polyfit(x,y,n)`<div align = 'right'>            #returns coefficients of polynomial expression of order n</div>\n",
    "\n",
    "`p = poly1d(f)`                   <div align = 'right'> #returns polynommial expression with passed coefficients (here, f)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9264b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.polyfit(df['that_1_feature'], df['target_variable'], 3)           #say, n = 3\n",
    "p = poly1d(f)\n",
    "\n",
    "#now feed value of feature to p and it will predict/produce result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465937b4",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc44b9c",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge Regressioin is employed in a Multiple Regression Problem when <b>Multicolinearity</b> occurs i.e. when the model includes multiple factors that are not only related to the target variable but also to each other.<br><br>\n",
    "\n",
    "Ridge regression is an <i>extension of Linear regression</i> where the loss function is modified to minimize the complexity of the model.<br> \n",
    "This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.\n",
    "<br><br>\n",
    "<b>Loss function = OLS + alpha * summation (squared coefficient values)</b>\n",
    "<br><br>\n",
    "In the above loss function, alpha is the parameter we need to select.<br>\n",
    "A low alpha value can lead to over-fitting, whereas a high alpha value can lead to under-fitting.\n",
    "<br><br>\n",
    "\n",
    "ex: `1.x + 12.x^2 + ... + 5.x^8`<br>\n",
    "here, even for x = 2, x^8 is too big a number.<br>\n",
    "So, we modify the equation to <br>\n",
    "`(0.01).*1*x + (0.01)*12*x^2 + ..... + (0.01)*5*x^8` which generates a smaller output even for larger x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158af41",
   "metadata": {},
   "source": [
    "`n = 3                                           #some int val`<br>\n",
    "`pr = PolynomialFeatures(degree = n)             # polynomial regression object`<br>\n",
    "\n",
    "`x_train_pr = pr.fit_transform(x_train)\n",
    "x_test_pr  = pr.fit_transform(x_test)            #polynomial fit features (test and train dataset for ridge regression)` \n",
    "\n",
    "`RR = Ridge(alpha = 0.01)                        #Ridge Regression object with passed alpha value`<br>\n",
    "`RR.fit(x_train_pr, y_train)                     #Fit Ridge Regression model`<br>\n",
    "`yhat = RR.predict(x_test_pr)                    #Predict using Ridge Regression`<br>\n",
    "`RR.score(x_train_pr, y_train)                   #Evaluate Ridge Regression Model`<br>\n",
    "\n",
    "<b>NOTE :</b> Ridge Regression is quite common with Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PolynomialFeatures(degree=2)\n",
    "\n",
    "x_train_pr = pr.fit_transform(x_train)\n",
    "x_test_pr  = pr.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RR = Ridge(alpha=0.01)\n",
    "\n",
    "RR.fit(x_train_pr, y_train)\n",
    "\n",
    "yhat = RR.predict(x_test_pr)\n",
    "\n",
    "print(yhat[:4])\n",
    "print(y_test.values[:4])\n",
    "\n",
    "RR.score(x_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898b33c",
   "metadata": {},
   "source": [
    "#### tune hyperparameters (alpha) for Ridge Regression\n",
    "\n",
    "use <b>GridSearch</b> for finding the best `alpha` for Ridge Regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71439227",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dict()\n",
    "grid['alpha'] = [0.01, 0.03, 0.1, 0.3, 1]\n",
    "search = GridSearchCV(RR, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "results=search.fit(x_data, y_data)\n",
    "print('Best Estimate for Hyper Parameter Tuning : ', results.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##repeat above step with finer values\n",
    "#ex: if best estimator is 0.01, repeat again with grid['alpha'] = range(0.0,0.15,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model:\n",
    "\n",
    "RR = Ridge(alpha= results.best_estimator_)\n",
    "x_pr = pr.fit_transform(x_data)\n",
    "RR.fit(x_pr, y_data)\n",
    "\n",
    "pred_ridge = RR.predict(x_pr, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216de71",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46ec24",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "<b>R square value</b>: “(total variance explained by model) / total variance.”<br>\n",
    "Ideal R^2 value is 1, the closer it is to 1 the  better the model fits the data<br>\n",
    "`r2_score = reg_obj.score(x_data, y_data)\n",
    "print(\"Coefficient of determination R^2 of prediction: {:.4f}\".format(r2_score))\n",
    "`<br><br>\n",
    "\n",
    "\n",
    "<b>Root Mean Square Error (RMSE) Value</b>: square root of average of the square of the errors(predicted_val - actual_val)<br>\n",
    "The lower the MSE value, the better the model fits the data<br>\n",
    "`y_pred = reg_obj.predict(x_data)\n",
    "mse = mean_squared_error(y_data, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE value: {:.4f}\".format(rmse))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Simple Linear Regression\n",
    "\n",
    "for col in x_data.columns:\n",
    "    slr.fit(x_train[col], y_train)\n",
    "\n",
    "    pred = slr.predict(x_test[col])\n",
    "    \n",
    "    print('Score :', slr.score(x_test[col], y_test))\n",
    "    print('RMSE  :', np.sqrt(mean_squared_error(y_data, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26849c6b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e0166",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Multiple Linear Regression\n",
    "\n",
    "r2_score_mlr = mlr.score(y_test, pred_mlr)\n",
    "\n",
    "rmse_mlr  = np.sqrt(mean_squared_error(y_test, pred_mlr))\n",
    "\n",
    "print('Multiple Linear Regression R^2 score   :', r2_score_mlr)\n",
    "print('Multiple Linear Regression RMSE value :', rmse_mlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb31d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Polynomial Regression\n",
    "\n",
    "r2_score_poly = lin_reg2.score(y_data, pred_poly)\n",
    "\n",
    "rmse_poly     = np.sqrt(mean_squared_error(y_data, pred_poly))\n",
    "\n",
    "print('Polynomial Regression R^2 score   :', r2_score_poly)\n",
    "print('Polynomial Regression RMSE value :', rmse_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f24da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ridge Regression\n",
    "\n",
    "r2_score_ridge = RR.score(y_data, pred_ridge)\n",
    "\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_data, pred_ridge))\n",
    "\n",
    "print('Ridge Regression R^2 score   :', r2_score_ridge)\n",
    "print('Ridge Regression RMSE value :', rmse_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e4c9b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae62e31",
   "metadata": {},
   "source": [
    "## Cross Validation Score\n",
    "\n",
    "`from sklearn.model_selection import cross_val_score` \n",
    "<br>\n",
    "splits dataset into training and testing set 'cv = n' number of times and evaluates the score by cross-validation, <br>\n",
    "returns an array of scores, 1 for every cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mlr = cross_val_score(mlr, x_data, y_data, cv = 4)\n",
    "print(scores_mlr)\n",
    "\n",
    "score_mlr = scores_mlr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b60368",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_poly = cross_val_score(poly_reg, x_data, y_data, cv = 4)\n",
    "print(scores_poly)\n",
    "\n",
    "score_poly = scores_poly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9920df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ridge = cross_val_score(RR, x_data, y_data, cv = 4)\n",
    "print(scores_ridge)\n",
    "\n",
    "score_ridge = scores_ridge.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f31c4b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4de775",
   "metadata": {},
   "source": [
    "## Models comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores = [r2_score_mlr, r2_score_poly, r2_score_ridge]\n",
    "rmse_scores = [rmse_mlr, rmse_poly, rmse_ridge]\n",
    "cross_val_scores  = [score_mlr, score_poly, score_ridge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b922ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_comparison = pd.DataFrame([r2_scores, rmse_scores, cross_val_scores], \n",
    "                                 index = ['Multiple Linear Regression', 'Polynomial Regression', 'Ridge Regression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709fe28",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63977f",
   "metadata": {},
   "source": [
    "## Model evaluation with Visualization\n",
    "\n",
    "We will use <b>Regression Plot</b>. <br>\n",
    "What we look for is a mostly symmetrical distribution with points that tend to cluster towards the middle of the plot, ideally around smaller numbers of the y-axis. <br>\n",
    "If we observe some kind of structure that does not coincide with the plotted line, we have failed to capture the behavior of the data and should either consider some feature engineering, selecting a new model, or an exploration of the hyperparameters.<br><br>\n",
    "\n",
    "We also use <b>Residual Plot</b><br>\n",
    "Residuals are differences between the one-step-predicted output from the model and the measured output from the validation data set. <br>\n",
    "Thus, residuals represent the portion of the validation data not explained by the model. <br>\n",
    "A residual plot has the Residual Values on the vertical axis; the horizontal axis displays the independent variable. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4de0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = np.array([(mlr(), 'Multiple Linear Regression'), (poly_reg(), 'Polynomial Regression'), (RR(), 'Ridge Regression')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Regression Plot\n",
    "##create a (3*1) subplot with each subplot plotting a regression plot of actual data vs data predicted by (mlr(), poly_reg(), RR())\n",
    "\n",
    "def regression_comparison(mods, features, target):\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "    for mod, ax in ((mods[0], ax1),(mods[1], ax2),(mods[2], ax3)):\n",
    "        predicted = cv.cross_val_predict(mod[0], X=features, y=target, cv=4)\n",
    "        ax.scatter(target, predicted, c='#F2BE2C')\n",
    "        ax.set_title('Prediction Error for %s' % mod[1])\n",
    "        ax.plot([target.min(), target.max()], [target.min(), target.max()], 'k--', lw=4, c='#2B94E9')\n",
    "        ax.set_ylabel('Predicted')\n",
    "    plt.xlabel('Measured')\n",
    "    plt.show()\n",
    "    \n",
    "regression_comparison(models, x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4115ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Residual Plot\n",
    "\n",
    "def residual_comparison(mods,features,target):\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "    plt.title('Plotting residuals using training (blue) and test (green) data')\n",
    "    for m, ax in ((mods[0], ax1),(mods[1], ax2),(mods[2], ax3)):\n",
    "        for feature in list(features):\n",
    "            splits = cv.train_test_split(features[[feature]], target, test_size=0.2)\n",
    "            X_tn, X_tt, y_tn, y_tt = splits\n",
    "            m[0].fit(X_tn, y_tn)\n",
    "            ax.scatter(m[0].predict(X_tn),m[0].predict(X_tn)-y_tn,c='#2B94E9',s=40,alpha=0.5)\n",
    "            ax.scatter(m[0].predict(X_tt), m[0].predict(X_tt)-y_tt,c='#94BA65',s=40)\n",
    "        ax.hlines(y=0, xmin=0, xmax=100)\n",
    "        ax.set_title(m[1])\n",
    "        ax.set_ylabel('Residuals')\n",
    "    plt.xlim([20,70])        # Adjust according to your dataset\n",
    "    plt.ylim([-50,50])  \n",
    "    plt.show()\n",
    "\n",
    "residual_comparison(models, x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46074a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "026da61c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee0dfd",
   "metadata": {},
   "source": [
    "### Now that we have compared the 3 models using evaluation metrics and visual intuition, select the best fitting model for further evaluation and ultimately using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45043851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model():\n",
    "    \n",
    "    all_models = ['Multiple Linear Regression', 'Polynomial Regression', 'Ridge Regression']\n",
    "\n",
    "    selected_model = input('Select best fitting model from \\'Multiple Linear Regression\\', \\'Polynomial Regression\\', \\'Ridge Regression\\'\\n')\n",
    "\n",
    "    if selected_model not in all_models:\n",
    "        print('Please Select best fitting model from \\'Multiple Linear Regression\\', \\'Polynomial Regression\\', \\'Ridge Regression\\'\\n')\n",
    "        select_model()\n",
    "        \n",
    "    return selected_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dic = {'Multiple Linear Regression' : mlr(), 'Polynomial Regression' : poly_reg(), 'Ridge Regression' : RR()}\n",
    "\n",
    "best_model_str = select_model()\n",
    "\n",
    "model  = model_dic[best_model_str]\n",
    "y_pred = model.predict(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0587703",
   "metadata": {},
   "source": [
    "##ipywidgets for model selection<br>\n",
    "##doesn't work with jupyter notebook but does so in .py scripts<br><br>\n",
    "\n",
    "`\n",
    "import inquirer`<br><br>\n",
    "`\n",
    "##prompt user for selecting from a finite list of options\n",
    "`<br>\n",
    "`\n",
    "questions = [inquirer.List('regression_model', message = 'Select the best Regression Model as per Model Perfomance', choices = ['Multiple Linear Regression', 'Polynomial Regression', 'Ridge Regression'])]\n",
    "`<br><br>\n",
    "`\n",
    "answers = inquirer.prompt(questions)\n",
    "best_model_str = answers('regression_model')\n",
    "`<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece5751",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f1fb5",
   "metadata": {},
   "source": [
    "## Overfitting/Underfitting\n",
    "<br>We check for Overfitting/Underfitting using <b>Residual Plot</b> and <b>Histogram</b><br>\n",
    "\n",
    "We have already seen the residuals of our model while evaluating it with visual intuition and so, need only plot the<br><br>\n",
    "<b>Histogram</b><br>\n",
    "We plot bivariate histograms (Predicted and Observed Values) to show distributions of datasets.<br>\n",
    "If the histograms do not overlap much it suggests the  model Underfits the data, and if they completely overlap, it suggests that the model Overfits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram:\n",
    "\n",
    "a1= sns.histplot(y_data, bins=40, color = 'palegreen')\n",
    "a2= sns.histplot(y_pred, bins=40, color = 'peachpuff', ax=a1)\n",
    "plt.title('Actual v/s Predicted CO2 Emissions by cars Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aac7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model coefficient of determination  of the prediction i.e. R^2 score\n",
    "print('{m} Model coefficient of determination  of the prediction (R^2) : {s}'.format(a = best_model_str, s = model.score(x_data, y_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4dbb3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef564c8d",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "A machine learning pipeline helps to streamline and speed up the process by automating ML workflows and linking them together.\n",
    "<br><br>\n",
    "The purpose of a pipeline is to allow you to increase the iteration cycle with the added confidence that codifying the process gives and to scale how many models you can realistically maintain in production.<br><br>\n",
    " \n",
    "Most ML pipelines include these tasks:\n",
    "\n",
    "* Gathering data or drawing it from a data lake\n",
    "* Cleaning and preprocessing the data\n",
    "* Feature extraction and engineering\n",
    "* Creating the model with training data\n",
    "* Testing and validating the model <br><br>\n",
    "\n",
    "However, Pipelines can not be completely generalized as the model depends completely on the data and every data has it's own perks and noises.<br>\n",
    "\n",
    "Below is a rough schema for reference and application<br><br>\n",
    "\n",
    "<b>Assumption</b>: Data is cleaned: .dropna(), .astype(), Feature Engineering, etc are performed, the relationship of each independent variable with the dependent variable is known, and the final dataset x_data and y_data are being used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e5e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "    ('Scale1', RobustScaler(), ['col.s', 'which', 'require', 'robust', 'scaling']),\n",
    "    ('Scale2', StandardScaler(), ['col.s', 'which', 'require', 'standard', 'scaling']),\n",
    "    ('Cat_to_num', OneHotEncoding(), ['cat', 'var.s', 'to', 'be', 'converted']),\n",
    "    remainder = 'drop'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ac012",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('Preprocessing', ct),\n",
    "    ('Model', LinearRegression())       #or whatever model is found suitable\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6364f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use fit(), transform(), fit_transform(), _fit_predict(), predict(), etc according to the data and the model (i.e. with some context)\n",
    "\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "pipe.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9d6ef",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14eea6c",
   "metadata": {},
   "source": [
    "## Model Serialization\n",
    "\n",
    "Serialize the trained (and ready to use) Machine Learning Model using <b> pickle</b> or <b>joblib</b> (here, pickle)<br><br>\n",
    "\n",
    "The idea is that this python object converted to character stream contains all the information necessary to reconstruct the object in another python script i.e. we need not preprocess, analyze or train the data and model again. <br>\n",
    "We can directly read the serialized object and use it for predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Modle to file in the current working directory\n",
    "\n",
    "model.fit(x_data, y_data)               #train with entire dataset\n",
    "\n",
    "Pkl_Filename = \"whatever_filename_you_find_cute.pkl\"  \n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53289f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open('whatever_filename_you_find_cute.pkl', 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "\n",
    "#RR.predict(x_data, y_data) i.e. RR is the trained model which can be directly imported for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb9c6d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7a16f",
   "metadata": {},
   "source": [
    "## Interpretation and Conclusion<br>\n",
    "\n",
    "#### We interpret the relationship between variables, model and predicted outcomes and draw Conclusion from evaluation metrics and other fit means\n",
    "<br><br>\n",
    "\n",
    "Below are some possible Interpretiation and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79921ba7",
   "metadata": {},
   "source": [
    "\n",
    "1)It seemed like col_n and col_m were positively correlated, though the association was/not strong.<br><br>\n",
    "\n",
    "2)Higher col_n values does not increase the \\<target value>.\n",
    "<br><br>\n",
    "3)col_n is one of the factors for desired high outcome and (col_n is true) data have an increased/optimized outcome.\n",
    "<br><br>\n",
    "4)col_m (Categories of categorical variable) are irrelevant in optimizing \\<target value>\n",
    "<br><br>\n",
    "5)Regardless of data’s gender, etc, they have the same likelihood to expect certain outcome\n",
    "<br><br>\n",
    "6)col_m (Categories of categorical variable) variable was highly associated with \\<target value>.\n",
    "<br><br>\n",
    "7)col_m variable was highly associated with col_n.\n",
    "<br><br>\n",
    "8)Using \\<ML Model > we achieved <accuracy>% accuracy.<br><br><br><br>\n",
    "    \n",
    "The RMSE value has been found to be \\<rmse-val>. <br>\n",
    "It means the standard deviation for our prediction is \\<rmse-val>. So, sometimes we expect the predictions to be off by more than <rmse-val> and other times we expect less than <rmse-val>. <br><br>\n",
    "So, <b>the model is/not good fit to the data.</b><br><br><br>\n",
    "\n",
    "In business decisions, the benchmark for the R2 score value is 0.7. It means if R2 score value >= 0.7, then the model is good enough to deploy on unseen data whereas if R2 score value < 0.7, then the model is not good enough to deploy. <br><br>\n",
    "Our R2 score value has been found to be \\<r2_score>. It means that this model explains \\<r2_score*100> % of the variance in our dependent variable. So, <b>the R2 score value confirms that the model is/not good enough to deploy</b> because it does not provide good fit to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94b7ee",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7ffc0",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "<br>\n",
    "If a good fit to the model is found, Deploy it or create a Mobile or Desktop application using it. \n",
    "<br><br>\n",
    "Howewver, Deploying the model is beyond the scope of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
