{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cf38c3",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "<b>Logistic Reression</b> is a <b>Classification</b> algorithm for categorical variables i.e. for predicting discrete values ({0, 1}, {True, False}, etc) <br><br>\n",
    "\n",
    "The goal of a Logistic Regression model is to build a model to<br>\n",
    "- predict the class which the input sample case belongs to\n",
    "- find the probability of input sample case belonging to a class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85b62c",
   "metadata": {},
   "source": [
    "Logistic Regression is a variation of Linear Regression, used when the observed dependent variable, <b>y</b>, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n",
    "\n",
    "Logistic regression fits a special s-shaped curve by taking the linear regression function ${\\theta^TX}$ and transforming the numeric estimate into a probability with the following function, which is called the sigmoid function ùúé:\n",
    "\n",
    "\n",
    "$$\n",
    "‚Ñé\\_\\theta(ùë•) = \\sigma({\\theta^TX}) =  \\frac1{1 + e^{-(\\theta\\_0 + \\theta\\_1  x\\_1 + \\theta\\_2  x\\_2 +\\cdots)}}\n",
    "$$\n",
    "\n",
    "Or:\n",
    "$$\n",
    "ProbabilityOfaClass\\_1 =  P(Y=1|X) = \\sigma({\\theta^TX}) = \\frac{1}{1+e^{-\\theta^TX}}\n",
    "$$\n",
    "\n",
    "In this equation, ${\\theta^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\\sigma(\\theta^TX)$ is the sigmoid or [logistic function]\n",
    "<br><br>\n",
    "\n",
    "if ${\\theta^TX}$ is a large value, then $\\sigma({\\theta^TX})$ ‚âà 1 and, <br>\n",
    "if ${\\theta^TX}$ is a small value, then  $\\sigma({\\theta^TX})$ \n",
    "‚âà 0.<br>\n",
    "Thus, the predicted value always lies in the range (0,1)\n",
    "\n",
    "So, briefly, Logistic Regression passes the input through the logistic/sigmoid but then treats the result as a probability:\n",
    "\n",
    "<img\n",
    "src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/images/mod_ID_24_final.png\" width=\"400\" align=\"center\">\n",
    "\n",
    "The objective of the **Logistic Regression** algorithm, is to find the best parameters Œ∏, for $‚Ñé\\_\\theta(ùë•)$ = $\\sigma({\\theta^TX})$, in such a way that the model best predicts the class of each case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef312a",
   "metadata": {},
   "source": [
    "<b>NOTE</b> in this notebook, we are building a binary classification model.<br><br>\n",
    "However, Logistic Regression can be used just the same for multi-class classification using the <b>one-vs-all technique</b> i.e \n",
    "- Train a Logistic Regression classifier $‚Ñé\\_\\theta^i(ùë•)$ for each discrete class i to predict the probability that y = i\n",
    "- On a new input 'x', to make a prediction, run $‚Ñé\\_\\theta^i(ùë•)$ for all i = 1, 2, 3, .. i.e. all the classes and pick the class that maximizes $‚Ñé\\_\\theta^i(ùë•)$\n",
    "- Pick the class i that maximizes $‚Ñé\\_\\theta^i(ùë•)$\n",
    "<br><br>\n",
    "In Layman terms, \n",
    "- classify the sample input using binary classification by clumping all classes but 1 together, and the left out class as the 2 required classes for Binary Classification \n",
    "- finding the probability of sample belonging to the left out class,\n",
    "- repeating these steps while leaving out each class and finding the probabilty of sample data to belong to that class \n",
    "- finally classifying the output to the class that has the maximum probability of sample data belonging to that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7c4c2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "sns.set(style='white')\n",
    "sns.set(style='whitegrid', color_codes=True)\n",
    "from scipy import optimize\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data using pandas method\n",
    "\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eafc9ec",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c918043f",
   "metadata": {},
   "source": [
    "## 1.1 Logistic Regression from scratch\n",
    "\n",
    "<b> first, we will build the Logistic Regression algorithm from scratch without using the scikit-learn library for understanding the algorithm</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cbc29",
   "metadata": {},
   "source": [
    "### 1.1.1 sigmoid function\n",
    "\n",
    "recall that the logistic regression hypothesis is defined as:\n",
    "\n",
    "$$ h_\\theta(x) = g(\\theta^T x)$$\n",
    "\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as: \n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0aac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['relevant', 'columns', 'and', 'not', 'target_var']]\n",
    "y = df['target_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Objective : calculate sigmoid function given imput z\n",
    "    \n",
    "    Parameters :\n",
    "    z : array_like\n",
    "        The input to the sigmoid function. \n",
    "        This can be a 1-D vector or a 2-D matrix\n",
    "    \n",
    "    Returns:\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z,\n",
    "        as the sigmoid is computed element wise on z.\n",
    "    '''\n",
    "    \n",
    "    z = np.array(z)\n",
    "    \n",
    "    g = np.zeros(z.shape)\n",
    "    \n",
    "    g = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965b400",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9033d6a",
   "metadata": {},
   "source": [
    "### 1.1.2 Cost Function\n",
    "\n",
    "cost function in logistic regression is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right]$$\n",
    "\n",
    "and the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$\n",
    "element (for $j = 0, 1, \\cdots , n$) is defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    '''\n",
    "    Objective \n",
    "    ---------\n",
    "    Compute cost and gradient for Logistic Regression\n",
    "    Compute the cost of a particular choice of theta. Set J to cost.\n",
    "    Compute the partial dereivatives and set the grad to the partial derivatives \n",
    "    of the cost w.r.t each parameter in theta.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "            The parameters for Logistic Regression. \n",
    "            This is a vector of shape (n+1, )\n",
    "    \n",
    "    \n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n+1) where m is the total number \n",
    "        of data points and n is the total number of features.\n",
    "\n",
    "    y : array_like\n",
    "        Labels for the input. \n",
    "        This is a vector of shape (m, )\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    J : float\n",
    "        The computed version for the Cost Function\n",
    "        \n",
    "    grad : array_like\n",
    "           A vector of shape (n+1, ) which is the gradient of the cost function\n",
    "           w.r.t. theta, at the current values of theta.\n",
    "                \n",
    "    '''\n",
    "    \n",
    "    m = y.size\n",
    "    \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    z = np.dot(X, theta)\n",
    "    h = sigmoid(z)\n",
    "    \n",
    "    J = np.mean((- np.transpose(y)*np.log(h)) - (1-y)*np.log(1-h))\n",
    "    \n",
    "    grad = 1/m * np.dot(X.T, (h-y))\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4a4eb",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481f60a",
   "metadata": {},
   "source": [
    "### 1.1.3 find OPTIMAL learning parameters (ùúÉ)\n",
    "\n",
    "we can use different techniques like <b>Gradient Descent</b> to find the optimal values of parameter <b>theta</b>\n",
    "\n",
    "\n",
    "In this notebook, we will use the `scipy.optimize` module.<br>\n",
    "\n",
    "For Logistic Regression, we optimize the Cost Function J(ùúÉ) with parameters ùúÉ. Concretely, we use `optimize.minimize` to find the optimal parameters for the Logistic Regression cost function, given a fixed dataset (of X and y values).\n",
    "\n",
    "`scipy.optimize.minimize` takes in the following parameters:\n",
    "\n",
    "- `costFunctin` : A cost functin that given the training set and a particular ùúÉ, computes the logistic regression cost and gradient with respect to ùúÉ for the dataset (X,y). we only pass the name of this function and not the paranthesis, indicating that we are only providing reference to this function, and not evaluating the result from this function.\n",
    "\n",
    "- `initial_theta` : The inital values of parameters we are trying to optimize.\n",
    "\n",
    "- `(x, y)` : additional arguments to the cost function.\n",
    "\n",
    "- `jac` : Indication if the Cost Function returns the Jacobian (gradient) along with the cost value. (True)\n",
    "\n",
    "- `method` : Optimization method/arguments to use\n",
    "\n",
    "- `options` : Additional options which might be specific to the specific optimizaqtion method.\n",
    "\n",
    "If we write the `costFunction` correctly, `optimize.minimize` will converge on the right optimization parameters and return the final values of the cost and ùúÉ in a class object.\n",
    "<br><br>\n",
    "NOTE: using `scipy.optimize.minimize` we only had to pass in a correct Cost Function and not worry about the writing any for loops or setting a learning rate as we had to do in Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set options; set max number of iterations to 400\n",
    "options = {'maxiter': 400}\n",
    "\n",
    "res = optimize.minimize(costFunction, initial_theta, (X,y),\n",
    "                        jac=True, method='TNC', options=options)\n",
    "\n",
    "#fun property of 'OptimizeResult' obect resturns \n",
    "#the value of costFunctionat optimized theta\n",
    "cost = res.fun\n",
    "\n",
    "#the optimized theta is in the x property\n",
    "theta = res.x\n",
    "\n",
    "print('Cost at theta found by optimize.minimize: {:.3f}'.format(cost))\n",
    "\n",
    "print('theta:')\n",
    "print('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5044340",
   "metadata": {},
   "source": [
    "### 1.1.4 Visualize Decision Boundary\n",
    "\n",
    "use `seaborn` or `matplotlib` methods to Visualize data and the calclated optimal values for ùúÉ to plot the data and the Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303615b",
   "metadata": {},
   "source": [
    "### 1.1.5 Evaluate Logistic Regression\n",
    "\n",
    "\n",
    "After learning the parameters, we can can use the model to predict 0/1, True/False, etc given sample input(s) against a user-set threshold 9between 0 and 1) \n",
    "<br><br>\n",
    "for new Input 'X'\n",
    "\n",
    "$$\n",
    "       P(y=1 | x) = sigmoid(ùúÉ^T X) \n",
    "       $$\n",
    "\n",
    "i.e. probability that (ouptput) y=1 given 'X' is given by $sigmoid(ùúÉ^T X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    '''\n",
    "    Objective\n",
    "    ---------\n",
    "    Predict whether the label is 0 or 1 using learned Logistic Regression\n",
    "    Computes the predictions for X using a threshold at (here) 0.5 \n",
    "    (i.e., if sigmoid(theta.T*x) >= 0.5, predict 1)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "            Parameters for logistic regression.\n",
    "            A vector of shape (n+1, )\n",
    "    \n",
    "    X : array_like\n",
    "        The data to use for computing predictions.\n",
    "        The row is the number of points to compute predictions,\n",
    "        and columns is the number of features.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    p : array_like\n",
    "        Predicstions as 0 or 1 for each row in X.     \n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] #number of training examples\n",
    "    \n",
    "    p = np.zeros(m)\n",
    "    \n",
    "    p = (1 / (1+np.exp(np.dot(X, theta.T)))<0.5) * 1\n",
    "                         # <0.5 makes 'p' an ndarray of True and False\n",
    "                         # *1 transforms True/False values to 1/0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c6842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = predict(theta, X_test)\n",
    "#print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee02bd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14bcd0",
   "metadata": {},
   "source": [
    "### 1.1.6 Evaluate model performance\n",
    "\n",
    "described later in this notebook in details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcab18",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad8bbe",
   "metadata": {},
   "source": [
    "## 1.2 Regularized Logistic Regression\n",
    "\n",
    "<b>Regularization</b> a technique to solve the problem of overfitting by penalizing the cost function. <br>\n",
    "It does so by using an additional penalty term in the cost function called the <b>regularization parameter : Œª </b>\n",
    "<br>\n",
    "The larger lambda is, the more the coefficients are shrunk toward zero (and each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da18410",
   "metadata": {},
   "source": [
    "### 1.2.2 Feature Mapping \n",
    "\n",
    "\n",
    "When that dataset cannot be linearly separated, a straight-forward application of Logistic Regression will not perform well on the dataset since Logistic Regression will only be able to find a linear decision boundary.\n",
    "<br>\n",
    "\n",
    "One way to fit such dataset (which requires a non-linear decision boundary say, elliptical) is to create more features from each data point i.e. <b>add Polynomial features</b> to our matrix (similar to Polynomial Regression).\n",
    "<br>\n",
    "\n",
    "The function `mapFeature` defined in the `utils.py` allows us to map features into all polynomial terms up to 6th power.\n",
    "\n",
    "$$ \\text{mapFeature}(x) = \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_1 x_2 & x_2^2 & x_1^3 & \\dots & x_1 x_2^5 & x_2^6 \\end{bmatrix}^T $$\n",
    "\n",
    "As a result, our logistic regression classifier will be trained on higher-dimension feature vector and have a more complex decision boundary and will appear non-linear when drawn in our 2-dimensional plot.<br><br>\n",
    "\n",
    "While feature mapping allows us to build a more expressive classifier, it also makes the model more susceptible to Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8120f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['relevant', 'columns', 'and', 'not', 'target_var']]\n",
    "y = df['target_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mapFeature(X, degree=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1399d2",
   "metadata": {},
   "source": [
    "### 1.2.3 Regularized Cost Function and Gradient\n",
    "\n",
    "Regularized cost function in logistic regression is\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\theta \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\theta \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 $$\n",
    "\n",
    "Note that you should not regularize the parameters $\\theta_0$. The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\qquad \\text{for } j \\ge 1 $$\n",
    "<a id=\"costFunctionReg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8219743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction_Reg(theta, X, y, lambda_):\n",
    "    '''\n",
    "    Objective\n",
    "    ---------\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    Compute the cost `J` of a particular choice of theta.\n",
    "    Compute the partial derivatives and set `grad` to the partial\n",
    "    derivatives of the cost w.r.t. each parameter in theta.\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array_like\n",
    "            Logistic regression parameters. A vector with shape (n, ). \n",
    "            n is the number of features including any intercept. \n",
    "            If we have mapped our initial features into polynomial features, \n",
    "            then n is the total number of polynomial features. \n",
    "                \n",
    "    X : array_like\n",
    "        The dataset with the shape (m x n)\n",
    "        m is the number of data points (examples)\n",
    "        n is the number of features (After feature mapping)\n",
    "        \n",
    "    y : array_like\n",
    "        The data labels. A vector with shape(m, )\n",
    "        \n",
    "    lambda_ : float\n",
    "              The regularization parameter                \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the regularized cost function\n",
    "        \n",
    "    grad : array_like\n",
    "           A vector of shape (n, ) which is the gradient of the cost function\n",
    "           w.r.t. each parameter in theta \n",
    "    '''\n",
    "    \n",
    "    m = y.size\n",
    "    \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    z = np.dot(X, theta)\n",
    "    h = sigmoid(z)\n",
    "    \n",
    "    J = np.mean((- np.transpose(y)*np.log(h)) - (1-y)*np.log(1-h)) + (lambda_/(2*m))*np.mean(grad[1:])\n",
    "    reg = (lambda_/(2*m)) * (theta[1:].T@theta[1:])\n",
    "    J += reg\n",
    "    \n",
    "    grad = (1/m) * X.T @ (h-y)\n",
    "    grad[1:] = grad[1:] + (lambda_ / m) * theta[1:]\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246732d",
   "metadata": {},
   "source": [
    "Once we are done with the costFunctionReg, we call it below using the initial value of  ùúÉ  (initialized to all zeros), and also another test case where  ùúÉ  is all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ff5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros(X.shape[1])\n",
    "lambda_ = 1\n",
    "\n",
    "# Compute and display initial cost and gradient for regularized logistic\n",
    "# regression\n",
    "cost, grad = costFunctionReg(initial_theta, X, y, lambda_)\n",
    "\n",
    "print('Cost at initial theta (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe2df9",
   "metadata": {},
   "source": [
    "### 1.2.4 Learning parameters using `scipy.optimize.minimize`\n",
    "\n",
    "Similar to the previous parts, we will use `optimize.minimize` to learn the optimal parameters <b>ùúÉ</b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee28fb2",
   "metadata": {},
   "source": [
    "### 1.2.5 Visualize Decision Boundary\n",
    "\n",
    "use `seaborn` or `matplotlib` methods to Visualize data and the calclated optimal values for ùúÉ to plot the data and the Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125be06",
   "metadata": {},
   "source": [
    "### 1.2.5 Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86385904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'theta' is the optimized learning parameters optimized using 'optimize.minimize as done above'\n",
    "\n",
    "#pred2 = predict(theta, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e8f51c",
   "metadata": {},
   "source": [
    "### 1.2.6 Evaluate model performance\n",
    "\n",
    "described later in this notebook in details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a447383",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1b623",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0599f",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression with python\n",
    "\n",
    "using `pandas`, `scikit-learn` and other libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4c70d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47192477",
   "metadata": {},
   "source": [
    "### 2.1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69692ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "#OR\n",
    "df = pd.read_csv(http://url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751ba55",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540e96e",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis\n",
    "\n",
    "It is the approach of analyzing data sets to summarize their main characteristics.\n",
    "<br><br>\n",
    "In Logistic Regression, we find the underlying story the data tells by <b>Visualizing</b> the data.\n",
    "<br>\n",
    "In other words, we find the <b>main features</b> of the dataset by plotting all the features against the target variable and interpreting the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecf173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7150a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, 'unique values :', df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0fd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6793e5",
   "metadata": {},
   "source": [
    "### 2.2.2 Grouping Data\n",
    "\n",
    "Pandas `dataframe.groupby()` function is used to split the data into groups based on some criteria. <br>\n",
    "Pandas objects can be split on any of their axes. (row/col) <br>\n",
    "The abstract definition of grouping is to provide a mapping of labels to group names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_players = df.group_by(['Team', 'Position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_perfomance = df.group_by(['Team'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e3aacc",
   "metadata": {},
   "source": [
    "for grouping 3 variables, `pandas.pivot(index, columns, values)` function produces <b>pivot table</b> based on 3 columns of the DataFrame. Uses unique values from index / columns and fills with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d38364",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ex:\n",
    "\n",
    "import pandas as pd\n",
    "  \n",
    "pivot_df = pd.DataFrame({'A': ['John', 'Cena', 'Mina'],\n",
    "      'B': ['Masters', 'Masters', 'Graduate'],\n",
    "      'C': [27, 23, 21]})\n",
    "  \n",
    "print(pivot_df, '\\n\\n')\n",
    "print(pivot_df.pivot('A', 'B', 'C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dc4d6",
   "metadata": {},
   "source": [
    "### 2.2.3 Plot data\n",
    "\n",
    "Plot each feature (variable) against the target variable<br><br>\n",
    "\n",
    "Use **countplot** and **barplot** to understand if features are relevent to the target variable, and <br>\n",
    "**boxplots** to understand the relationship of features among themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90858805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplot is for comparison against continuous values\n",
    "sns.barplot('feature1', 'target_var', data=df, color='darkturqoise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "temp_data = df[['continuous_feature', 'target_var']].groupby(['continuous_feature'], as_index=False).mean()\n",
    "g = sns.barplot(x='conitnuous_feature', y='target_var', data=temp_data, color='LightSeaGreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#countplot is similar to barplot, it is for comparison against categorical values\n",
    "sns.countplot(x='feature', hue='target_var', data=df, palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620be18",
   "metadata": {},
   "source": [
    "## 2.3 clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ca93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with missing values in dataset\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "#if data is missing in columns:\n",
    "#1. check the pattern of data by plotting a histogram\n",
    "#2. check %age of missing data i that column\n",
    "#3. check mean and median of that column\n",
    "#4. replace missing values with appropriate value (mean/median/etc) \n",
    "#5. drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45997664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let, col1, col2, col3 have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of missing col1 is %.f' %(df['col1'].isnull().sum()/df.shape[0] * 100))\n",
    "\n",
    "ax = df['col1'].hist(bins=15, density=True, stacked=True, color='teal')\n",
    "df['col1'].plot(kind='density', color='teal')\n",
    "ax.set(xlabel='col1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean col1\n",
    "print('The mean of \"col1\" is %.2f' %(df[\"col1\"].mean(skipna=True)))\n",
    "# median col1\n",
    "print('The median of \"col1\" is %.2f' %(df[\"col1\"].median(skipna=True)))\n",
    "\n",
    "#suppose this value is less (<20%) and this feature has high correlation with the target variable, \n",
    "#then we replace all the missing values with the mean or median of this column\n",
    "#(depending on the distribution of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize distribution of continuous features to understand how to deal with missing values\n",
    "\n",
    "df['col1'].hist(color='green', bins=20, figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of missing col2 is %.f' %(df['col2'].isnull().sum()/df.shape[0] * 100))\n",
    "#suppose this %age is high, so we drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of missing col3 is %.f' %(df['col3'].isnull().sum()/df.shape[0] * 100))\n",
    "#suppose this %age is less, then\n",
    "\n",
    "print(df['col3'].value_counts())\n",
    "sns.countplot(x='col3', data=df, palette='Set2')\n",
    "plt.show()\n",
    "#this will print and show the most occuring values in 'col3'\n",
    "\n",
    "print('The most common boarding port of col3 is %s.' %df['col3'].value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use bosplots to understand the relationship of features with each other\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x='feature1', y='feature12', data=df, palette='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c4534",
   "metadata": {},
   "source": [
    "### 2.3.1 adjustments to dataset for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe34213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col1'].fillna(df['col1'].median(skipna=True), inplace=True)\n",
    "\n",
    "df.drop('col2', axis=1, inplace=True)\n",
    "\n",
    "df['col3'].fillna(df['col3'].value_counts().idxmax(), inplace=True)\n",
    "\n",
    "df['col4'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with all columns with missing values in the aforementioned way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36852fd",
   "metadata": {},
   "source": [
    "### 2.3.2 change dtypes of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['col1', 'col_n']] = df[['col1', 'col_n']].astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186f104",
   "metadata": {},
   "source": [
    "### 2.3.3 cleaning column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166090e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24820b3",
   "metadata": {},
   "source": [
    "### 2.3.4 Dealing with copy variables with case sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ffad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.columns)-1):\n",
    "    if df.dtypes[i] == 'object':\n",
    "        df[datdfaset.columns[i]] = dataset[dataset.columns[i]].str.lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27163361",
   "metadata": {},
   "source": [
    "### 2.3.5 Bin data\n",
    "\n",
    "Binning is the process of grouping values together into 'bins'\n",
    "ex: bin 'age' into 'kid', 'teen,', 'adult', 'middle-aged', etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54843d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(df['col_to_be_binned']), max(df['col_to_be_binned']), 4)\n",
    "#this divides the 'col_to_be_binned' into 4 bins w.r.t their min and max val\n",
    "\n",
    "bin_names = ['bin1_small', 'bin2_medium', 'bin3_large']\n",
    "#i.e. a list of bin names\n",
    "\n",
    "df['binned_col'] = pd.cut(df['col_to_be_binned'], bins, label = bin_names, include_lowest = True)\n",
    "\n",
    "#this creates a new col in the DataFrame with binned data\n",
    "#you can delete the  original column which was binned to reduce dataframe size\n",
    "\n",
    "df.drop(['col_to_be_binned'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11113f0",
   "metadata": {},
   "source": [
    "## 2.4 Categorical Variables into Numerical Variables\n",
    "\n",
    "Categorical Variables (ex: df['sex'].unique() = ['M', 'F'] needs to be converted into Numeircal Variables to be included in the ML algorithm<br><br>\n",
    "\n",
    "This can be achieved by:\n",
    "\n",
    "1. <b>Dummy Variables</b> :  A dummy variable is a binary variable that indicates whether a separate categorical variable takes on a specific value.\n",
    "In our example, it will create 2 columns and provide each row w with [0, 1] if data is Male and [1, 0] if data is Female.<br>\n",
    "Dummy Variable works well while you are exploring and analyzing but for final inclusion in the dataset, OneHotEncoding is much more suitable.<br><br>\n",
    "\n",
    "2. <b>One Hot Encoding</b> : It works essentially the same as Dummy Vairables the only difference is that it excludes 1 row. This works with the concept that the variables have a linear relationship and hence for n unique categories in categorical variable, there are n-1 new vairables. \n",
    "In our example, it will create 1 column and provide each row w with [0] if data is Male and [1] if data is Female.<br><br>\n",
    "\n",
    "3. <b>Label Encoding</b> : It follows Ordinal Scaling i.e. for each category it establishes a relationship among the categories as per their ranking.<br>\n",
    "ex: for categories : ['low', 'medium', 'high'], it will be twice and thrice as affective for 'medium' and 'high' respectively than for 'low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dummy Vairables\n",
    "\n",
    "dvar = pd.get_dummies(df['col_to_be_binned'], columns = ['cat_1', 'cat_n'])\n",
    "df = pd.concat([df, dvar], axis = 1)\n",
    "df.drop(df['col_to_be_binned'], axis=1, inplace=True)\n",
    "\n",
    "# you may drop 1 dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579822f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LabelEncoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()                               #labelencoderobject\n",
    "\n",
    "copy_df = df\n",
    "copy_df.cat_var = le.fit_transform(copy_df.cat_var)\n",
    "#this LeabelEncodes the 'cat_var' category of 'copy_df' DataFrame and puts it back in the 'cat_var' category of 'copy_df' DataFrame\n",
    "#i.e. all the categories in our categorical variables are converted into int numbers (0,1,2,..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OneHotEncoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()                             #onehotencoder object\n",
    "\n",
    "encoded_cat_col = ohe.fit_transform(copy_df.cat_var).toarray()\n",
    "#onehotencoded_col = ohe.fit_Transform(label_encoded_col)\n",
    "df = df.join(encoded_cat_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2546f3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f10695d",
   "metadata": {},
   "source": [
    "## 2.5 Feature selection (basic)\n",
    "**We now understand which features are relevant to the target variable and so will keep only those variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a01b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = df['target_var']\n",
    "x_data = df.drop(columns= ['target_var', \n",
    "                           'other_irrelevant_columns_as_shown_by_data/statistical_analysis'],\n",
    "                          inplace = True)\n",
    "\n",
    "##y_data is target data\n",
    "##x_data is parameters that affect the target data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1bd10",
   "metadata": {},
   "source": [
    "## 2.6 Data Standardzation\n",
    "\n",
    "Data standardization is the process of rescaling the attributes so that they have mean as 0 and variance as 1. <br>\n",
    "This refers to bringing down all the features to a common scale without distorting the differences in the range of the values.\n",
    "ex: np.arange(0.01,1,0.01), np.arange(1, 10000, 10)] to [np.arange(0.01,1,0.01), np.arange(0.01, 1, 0.01)]\n",
    "<br><br>\n",
    "For this, each data point in  the col = (data point - mean of col)/std of col<br><br>\n",
    "Though this is done easily using `sklearn.preprocessing.StandardScaler` , some additional context to the data is also required<br>\n",
    "ex: if col8: mileage(kmph) in city and col9: mileage(mpg) in highway, then first convert either mpg to kmph or vice versa and then StandardScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler  = preprocessing.StandardScaler()\n",
    "scaler2 = preprocessing.StandardScaler()\n",
    " \n",
    "standard_x_df = scaler.fit_transform(x_data)\n",
    "standard_y_df = scaler.fit_transform(y_data)\n",
    "\n",
    "X = pd.DataFrame(x_data)\n",
    "y = pd.DataFrame(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433224a",
   "metadata": {},
   "source": [
    "<b>Now that the data is cleaned and preprocessed and we have some understanding od the data, we we should proceed to model building</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2a68bd",
   "metadata": {},
   "source": [
    "## 2.7 Model Building  (Logistic Regression with Scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35a6fa",
   "metadata": {},
   "source": [
    "### 2.7.1 Feature Selection\n",
    "\n",
    "**Feature selection** refers to techniques that select a subset of the **most relevant features** (columns) for a dataset. Fewer features can allow machine learning algorithms to run more efficiently (less space or time complexity) and be more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab7888",
   "metadata": {},
   "source": [
    "#### (i) Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE is a wrapper-type feature selection algorithm. <br>\n",
    "This means that a different machine learning algorithm is given and used in the core of the method, is wrapped by RFE, and used to help select features. This is in contrast to filter-based feature selections that score each feature and select those features with the largest (or smallest) score.\n",
    "<br><br>\n",
    "\n",
    "There are two important configuration options when using RFE: \n",
    "- the choice in the number of features to select, and \n",
    "- the choice of the algorithm used to help choose features\n",
    "\n",
    "<br>\n",
    "\n",
    "**Recursive Feature Elimination (RFE)** is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. \n",
    "<br>\n",
    "The goal of RFE is to select features by recursively considering smaller and smaller sets of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#build Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "#create RFE model and select 8 attributes (no of attributes is arbitrary, refer later part of this notebook)\n",
    "rfe = RFE(estimator=model, n_features_to_select=8)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "#summarize the selction of attributes\n",
    "selected_features = list(X.columns[rfe.support_])\n",
    "print('Selected Features : ', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3aaec",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4791044",
   "metadata": {},
   "source": [
    "#### (ii) Feature ranking with recursive feature elimination and cross-validation (RFECV)\n",
    "\n",
    "RFECV performs RFE in a cross-validation loop to find the optimal number or the best number of features.\n",
    "<br>\n",
    "Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10595da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFCEV\n",
    "\n",
    "#create RFECV object and compute a cross-validation score\n",
    "#The 'accuracy' is proportional to the number of correct classifications\n",
    "\n",
    "rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\n",
    "rfcev.fit(X, y)\n",
    "\n",
    "#print optimal number of features and features selected\n",
    "print('Optimal number of features : %d' % rfecv.n_features_)\n",
    "selected_features = list(X.columns[rfecv.support_])\n",
    "print('Selected Features : %s' % selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot number of features VS. cross-validation scores\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of features selected')\n",
    "plt.ylabel('Cross Validation score (nb of correct classifications)')\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) +1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x = X[selected_features]\n",
    "\n",
    "#features' correlation heatmap\n",
    "plt.subplots(figsize=(8, 5))\n",
    "sns.heatmap(final_x.corr(), annot=True, cmap='RdY1Gn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efaf6e4",
   "metadata": {},
   "source": [
    "### 2.7.2 train_test_split\n",
    "\n",
    "`sklearn.model_selection.train_test_split()` splits arrays or matrices into random train and test subsets<br><br>\n",
    "Use the training set to train the Machine Learning model and the test set to evaluate it's perfomance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b47233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.modelSelection import train_test_split\n",
    "\n",
    "##y is target data\n",
    "##final_x is final dataset consisting of all sample data and only selected features\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(final_x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53593388",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade9cff",
   "metadata": {},
   "source": [
    "## 2.8 Predict result\n",
    "\n",
    "Use `LogisticRegression` model from `sklearn.linear_model` <br> <br>\n",
    "\n",
    "Use `predict()` method to classify sample data into discrete classes (0, 1, ..) <br> <br>\n",
    "\n",
    "`predict_proba()` method returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 0 P(Y=0|X), the second column is the probability of class 1 P(Y=1|X), and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f17030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting values froms ample data\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "#predict() method \n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "#predict_proba() method \n",
    "y_pred_probability = logreg.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f905808b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011662c3",
   "metadata": {},
   "source": [
    "## 2.9 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7265dc",
   "metadata": {},
   "source": [
    "### 2.9.1 jaccard index\n",
    "\n",
    "We can define jaccard as the size of the intersection divided by the size of the union of the two label sets.<br><br>\n",
    "If the entire set of predicted labels for a sample strictly matches with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "jaccard_score(y_test, y_pred, pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a8ea4",
   "metadata": {},
   "source": [
    "### 2.9.2 Log Loss\n",
    "\n",
    "**Log-loss** is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). <br>\n",
    "The more the predicted probability diverges from the actual value, the higher is the log-loss value.\n",
    "<br><br>\n",
    "\n",
    "Mathematical Meaning : <br>\n",
    "Log Loss is the negative average of the log of corrected predicted probabilities for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24542ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "print('Log Loss :', log_loss(y_test, y_pred_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687efd49",
   "metadata": {},
   "source": [
    "### 2.9.3 Confusion Matrix\n",
    "\n",
    "Another way of looking at the accuracy of the classifier is to look at **confusion matrix**.<br><br>\n",
    "\n",
    "Confusion Matrix is a (2 x 2) table that has the values <br>\n",
    "$$\n",
    "\\left(\\begin{array}{cc} \n",
    "True Positive  & False Positive\\\\\n",
    "False  Negative & True  Negative\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b05d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "print(confusion_matrix(y_test, y_pred, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['class=1','class=0'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403e870",
   "metadata": {},
   "source": [
    "### 2.9.4 Accuracy, Precision, Recall, F-measure and support\n",
    "\n",
    "<br>\n",
    "\n",
    "**Accuracy** is the proportion of correct predictions over total predictions.<br><br>\n",
    "\n",
    "**Precision** is the ratio $ tp / (tp + fp) $ where tp is the number of true positives and fp the number of false positives. <br>\n",
    "Precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    "<br><br>\n",
    "\n",
    "**Recall** is the ratio $ tp / (tp + fn) $ where tp is the number of true positives and fn the number of false negatives.<br>\n",
    "Recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "<br><br>\n",
    "\n",
    "**F-beta score** can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.<br>\n",
    "The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\n",
    "<br><br>\n",
    "    \n",
    "**Support** is the number of occurrences of each class in y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b748693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "print('Accuracy :', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7716b",
   "metadata": {},
   "source": [
    "### 2.9.5 ROC Curve and AUC\n",
    "\n",
    "An **ROC curve** (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. \n",
    "<br>This curve plots two parameters:\n",
    "- True Positive Rate (TPR) \n",
    "    $$ TPR = TP / (TP + FN) $$\n",
    "- False Positive Rate (FPR)\n",
    "    $$ FPR = FP / (FP + TN) $$\n",
    "    \n",
    "An ROC curve plots TPR vs. FPR at different classification thresholds. <br>\n",
    "Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives.\n",
    "<br><br>\n",
    "\n",
    "**AUC** or **Area Under the ROC Curve**  measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).<br><br>\n",
    "AUC provides an aggregate measure of performance across all possible classification thresholds.<br>\n",
    "AUC represents the probability that a random positive is positioned to the right of a random negative .<br>\n",
    "AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(x_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])\n",
    "print('Logistic Regression AUC is ', auc(fpr, tpr))\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294d02e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef723b4",
   "metadata": {},
   "source": [
    "## 2.10 Model Evaluation suing Cross-Validation\n",
    "\n",
    "**Cross-validation** is a resampling procedure used to evaluate machine learning models on a limited data sample using a single parameter called **k** that refers to the number of groups that a given data sample is to be split into, which is why the procedure is often called **k-fold Cross Validation**<br><br>\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "- Shuffle the dataset randomly.\n",
    "- Split the dataset into k groups\n",
    "- For each unique group:\n",
    "- (  i) Take the group as a hold out or test data set\n",
    "- ( ii) Take the remaining groups as a training data set\n",
    "- (iii) Fit a model on the training set and evaluate it on the test set\n",
    "- (iv) Retain the evaluation score and discard the model\n",
    "- Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "Model evaluation using k-fold Cross Validation can be done using 2 methods:\n",
    "\n",
    "1. `cross_val_score()` function\n",
    "2. `cross_validate()`  function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aac579",
   "metadata": {},
   "source": [
    "### 2.10.1  Model evaluation based on K-fold cross-validation using `cross_val_score()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5feb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold Cross Validation using Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "scores_accuracy = cross_val_score(logreg, final_x, y, cv=10, scoring='accuracy')\n",
    "\n",
    "scores_log_loss = cross_val_score(logreg, final_x, y, cv=10, scoring='neg_log_loss')\n",
    "\n",
    "scores_auc      = cross_val_score(logreg, final_x, y, cv=10, scoring='roc_auc')\n",
    "\n",
    "print('K-fold Cross-Validation results : ')\n",
    "\n",
    "print('Average accuracy is :', scores_accuracy.mean())\n",
    "print('Average log-loss is :', scores_log_loss.mean())\n",
    "print('Average auc is      :', scores_auc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa57b19",
   "metadata": {},
   "source": [
    "### 2.10.2  Model evaluation based on K-fold cross-validation using `cross_validate()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91099c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = {'accuracy': 'accuracy', 'log_loss': 'neg_log_loss', 'auc': 'roc_auc'}\n",
    "\n",
    "modelCV = LogisticRegression()\n",
    "\n",
    "results = cross_validate(modelCV, final_x, y, cv=10, scoring=list(scoring.values()), \n",
    "                         return_train_score=False)\n",
    "\n",
    "print('K-fold cross-validation results :')\n",
    "for sc in range(len(scoring)):\n",
    "    print(modelCV.__class__.__name__+\" average %s: %.3f (+/-%.3f)\" % (list(scoring.keys())[sc], -results['test_%s' % list(scoring.values())[sc]].mean()\n",
    "                               if list(scoring.values())[sc]=='neg_log_loss' \n",
    "                               else results['test_%s' % list(scoring.values())[sc]].mean(), \n",
    "                               results['test_%s' % list(scoring.values())[sc]].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29ebe9",
   "metadata": {},
   "source": [
    "## 2.11 GridSearchCV evaluating using multiple scorers simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0229da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': np.arange(1e-05, 3, 0.1)}\n",
    "scoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(), return_train_score=True,\n",
    "                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n",
    "\n",
    "gs.fit(final_x, y)\n",
    "results = gs.cv_results_\n",
    "\n",
    "print('='*20)\n",
    "print(\"best params: \" + str(gs.best_estimator_))\n",
    "print(\"best params: \" + str(gs.best_params_))\n",
    "print('best score:', gs.best_score_)\n",
    "print('='*20)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n",
    "\n",
    "plt.xlabel(\"Inverse of regularization strength: C\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xlim(0, param_grid['C'].max()) \n",
    "ax.set_ylim(0.35, 0.95)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results['param_C'].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n",
    "        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "        ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n",
    "        \n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score,\n",
    "                (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c988336",
   "metadata": {},
   "source": [
    "## 2.12 GridSearchCV evaluating using multiple scorers, RepeatedStratifiedKFold and pipeline for preprocessing simultaneously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3219c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Define simple model\n",
    "C = np.arange(1e-05, 5.5, 0.1)\n",
    "scoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "#Simple pre-processing estimators\n",
    "std_scale = StandardScaler(with_mean=False, with_std=False)\n",
    "\n",
    "#Defining the CV method: Using the Repeated Stratified K Fold\n",
    "n_folds=5\n",
    "n_repeats=5\n",
    "\n",
    "rskfold = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=n_repeats, random_state=2)\n",
    "\n",
    "#Creating simple pipeline and defining the gridsearch\n",
    "log_clf_pipe = Pipeline(steps=[('scale',std_scale), ('clf',log_reg)])\n",
    "\n",
    "log_clf = GridSearchCV(estimator=log_clf_pipe, cv=rskfold,\n",
    "              scoring=scoring, return_train_score=True,\n",
    "              param_grid=dict(clf__C=C), refit='Accuracy')\n",
    "\n",
    "log_clf.fit(final_x, y)\n",
    "results = log_clf.cv_results_\n",
    "\n",
    "print(\"best params: \" + str(log_clf.best_estimator_))\n",
    "print(\"best params: \" + str(log_clf.best_params_))\n",
    "print('best score:', log_clf.best_score_)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\",fontsize=16)\n",
    "\n",
    "plt.xlabel(\"Inverse of regularization strength: C\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xlim(0, C.max()) \n",
    "ax.set_ylim(0.35, 0.95)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results['param_clf__C'].data, dtype=float)\n",
    "\n",
    "for scorer, color in zip(list(scoring.keys()), ['g', 'k', 'b']): \n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = -results['mean_%s_%s' % (sample, scorer)] if scoring[scorer]=='neg_log_loss' else results['mean_%s_%s' % (sample, scorer)]\n",
    "        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "        ax.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "    best_score = -results['mean_test_%s' % scorer][best_index] if scoring[scorer]=='neg_log_loss' else results['mean_test_%s' % scorer][best_index]\n",
    "        \n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax.annotate(\"%0.2f\" % best_score,\n",
    "                (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d371010",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532945b4",
   "metadata": {},
   "source": [
    "## 2.13 predicting result of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let the fresh data be in a dataframe 'new_data'\n",
    "\n",
    "predicted = pd.concat(new_data['id_feature'], pd.Series(data=log_clf.predict(new_data[selected_features])))\n",
    "#so the 'predicted' dataframe has 2 columns: id_feature' to identify each data sample uniquely and \n",
    "#outcome' which is the class in which each data sample is classified respectively\n",
    "\n",
    "predicted.to_csv('new_predicted_outcomes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c50b46",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de973c5b",
   "metadata": {},
   "source": [
    "## 2.14 Serialize model using pickle module\n",
    "\n",
    "Serialize the trained (and ready to use) Machine Learning Model using `pickle` or `joblib` (here, pickle)\n",
    "<br><br>\n",
    "\n",
    "The idea is that this python object converted to character stream contains all the information necessary to reconstruct the object in another python script i.e. we need not preprocess, analyze or train the data and model again.<br>\n",
    "We can directly read the serialized object and use it for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Modle to file in the current working directory\n",
    "\n",
    "log_clf.fit(final_x, y)\n",
    "\n",
    "Pkl_Filename = \"whatever_filename_you_find_cute.pkl\"  \n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file\n",
    "with open('whatever_filename_you_find_cute.pkl', 'rb') as file:  \n",
    "    model = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
