{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4737fef3",
   "metadata": {},
   "source": [
    "# Decision Tree Skeleton\n",
    "\n",
    "**Decision Trees are a non-parameteric Supervised Learning method for both Classification and Regression** <br>\n",
    "\n",
    "*non-parameteric Machine Learning algorithms are algorithms that do not make strong assumptions about the form of mapping function<br>\n",
    "for eaxmple, Logistic Regression is a parameteric Machine Learning algorithms that assumes $y = \\frac{1}{1 + \\exp^{-\\theta^{T} X}}$ and tries to optimize $\\theta$ <br>\n",
    "non-parameteric Machine Learning algorithms such as Decision Tree does not presume any known/simplified function*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0f9a9",
   "metadata": {},
   "source": [
    "The basic intuition behind a Decision Tree is to map out all possible decision paths in the form of a tree<br>\n",
    "\n",
    "In general, Decision Trees are referred to as **CART** or **Classification and Regression Trees**<br>\n",
    "\n",
    "Decision Trees are **built using recursive partitioning** to classify the data segments by **minimizing the Impurity** at each step and **maximizing the Information Gain**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "166f4a3d",
   "metadata": {},
   "source": [
    "## Decision Tree Terminologies\n",
    "\n",
    "### Root Node:\n",
    "It represents the entire population or sample and this further gets divided into 2 or more homogeneous sets\n",
    "\n",
    "### Leaf Node:\n",
    "Node that cannot be further segregated into further nodes. This node generally has Pure subset\n",
    "\n",
    "### Splitting: \n",
    "It referes to dividing the root node/sub node into different parts on the basis of some condition\n",
    "\n",
    "### Branch / SubTree:\n",
    "Sub Tree formed by the splitting of Tree\n",
    "\n",
    "### Purity/Impurity: \n",
    "The concept of Im/purity is based on the fractin of data (or sample under consideration) that belongs to just 1 class<br>\n",
    "**Pure subset**   : subset that contains data belonging to only 1 class (8-A, 0-B) or (0-A, B-B) <br>\n",
    "**Impure subset** : subset that contains data belonging to multiple classes (6-A, 2-B), (4-A, 4-B), etc <br>\n",
    "A node having equally distributed split of classes (50-50 or 33-33-33) has the worst Purity <br><br>\n",
    "\n",
    "###  Entropy:\n",
    "It is the measure of randomness or uncertainity in data i.e. it is the measure of Purity in data (or sub-split) <br>\n",
    "*Lower the Entropy, less uniform the Distribution, ourer the node* <br>\n",
    "[1-A / 7-B]--> Low Entropy &emsp; [3-A / 5-B]-->High Entropy <br>\n",
    "If sample is completely homogeneous (contains data of only 1 class), the Entropy is 0 and if sample is equally divided (50-50, 33-33-33, etc), it has Entropy of 1 <br>\n",
    "[8-A / 0-B] or [0-A / 8-B]--> Entropy = 0 &emsp; [4-A / 4-B]-->Entropy = 1\n",
    "\n",
    "### Information Gain\n",
    "It is the information that can increase the level of uncertainity after splitting. Information Gain is in respect to the attribute which is under consideration <br>\n",
    "Information Gain quantifies how much a question/attribute (at node) reduces the Entropy (uncertainity) i.e ot measures how much information a feature(attribute) gives us about the class. <br>\n",
    "The feature with highest Informatin Gain is taken as split and the process is repeated until all children nodes are pure or Information Gain is 0<br>\n",
    "*Information Gain is the entropy of a tree before the split minus the weighted entropy after the split* <br>\n",
    "\n",
    "$ Information $ $ Gain = (Entropy$ $before$ $split) - (weighted$ $entropy$ $after$ $the$ $split) $\n",
    "\n",
    "Our objective is to minimize the uncertainity/randomness i.e. Entropy and maximize the Information Gain at every level in the Decision Tree <br>\n",
    "**So our goal is to find the attribute that has the highest Information Gain and use that attribute to split the tree in our Decision Tree**\n",
    "\n",
    "![infoGain.PNG](images/infoGain.PNG)\n",
    "\n",
    "In above example, Information Gain using attribute 'sex' is higher so we select 'sex' as node and continue to find Entropy and Information Gain with all other attributes to create the Decision Tree until purity of child node is 1 or Information Gain is 0\n",
    "\n",
    "### Gini Index\n",
    "Gini Index or Gini Score is a metric to measure how often a randomly chosen element would be incorrectly identified <br>\n",
    "Like Entropy, it is a criterion for calculating Information Gain <br>\n",
    "Gini Index quantifies the amount of uncertainity of a single node <br>\n",
    "*Attribute with lower Gini Index should be preferred for splitting the tree* <br>\n",
    "$ Gini Index = \\sum{(p * (1 - p) )} $ <br>\n",
    "where, p is the proportion of some class inputs present in a particular group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03f484",
   "metadata": {},
   "source": [
    "## How to build a Decision Tree\n",
    "A Decision Tree is constructed by considering all the attributes 1-by-1 <br>\n",
    "**STEPS**\n",
    "1. Choose an attribute from dataset\n",
    "2. Calculate the significance of attribute in splitting of data (Information Gain / Gini Index)\n",
    "3. Split data based in the most significant attribute\n",
    "4. go to step 1 i.e. go to each branch and repeat the steps for the rest of the attributes\n",
    "\n",
    "\n",
    "### How to pick features for splitting\n",
    "The significant attribute is selected by comparing the following measures of each attribute:\n",
    "\n",
    "1. Gini Index (low value desired)\n",
    "2. Information Gain (high value desired)\n",
    "3. Reduction in Variance (only in Regression problems - low value desired)\n",
    "4. Chi Square (low value desired)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0711a0a",
   "metadata": {},
   "source": [
    "### Advantages of CART\n",
    "\n",
    "- Simple to understand, interpret and visualize\n",
    "- Can handle both categorical and numerical data. Can also handle multi-output problems\n",
    "- Resistant to outliers, hence require little data preprocessing\n",
    "- Decision trees implicitly perform variable screening or feature selection\n",
    "- Nonlinear relationships between parameters do not affect tree performance\n",
    "\n",
    "### Disadvantages of CART\n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalize the data well. Thus they are prone to overfitting\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This is called *variance*, which needs to be lowered by methods like *bagging* and *boosting*\n",
    "- Decision tree learners create *biased trees* if some classes dominate. It is therefore recommended to balance the data set prior to fitting with the decision tree\n",
    "- Since Decision Tree learners fall under Greedy algorithms paradigm, they cannot gurantee to return the globally optimal model (decision tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c60ce2",
   "metadata": {},
   "source": [
    "## Avoid Overfitting\n",
    "\n",
    "### Pruning\n",
    "To avoid decision tree from overfitting we **remove the branches that make use of features having low importance**. This method is called as **Pruning** or **post-pruning**<br>\n",
    "This way **the complexity of tree is reduced, which improves the predictive accuracy by the reduction of overfitting**. <br>\n",
    "Pruning can start at either root or the leaves. <br>\n",
    "Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set\n",
    "\n",
    "\n",
    "### Early Stop\n",
    "An alternative method to prevent overfitting is to **try and stop the tree-building process early, before it produces leaves with very small samples**. This heuristic is known as **early stopping** but is also sometimes known as **pre-pruning** decision trees. <br>\n",
    "At each stage of splitting the tree, we check the cross-validation error. If the error does not decrease significantly enough then we stop. <br>\n",
    "Early stopping **may underfit** by stopping too early.<br>\n",
    "The current split may be of little benefit, but having made it, subsequent splits more significantly reduce the error <br><br>\n",
    "\n",
    "**NOTE:** Early Stopping and Pruning can be used together, separately, or not at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c58ffd",
   "metadata": {},
   "source": [
    "**NOTE:** In Machine Learning, **Ensemble methods** use (or combine) multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92865471",
   "metadata": {},
   "source": [
    "## Bagging and Boosting\n",
    "\n",
    "Decision Tree algorithms are highly prone to *Variance* i.e. a small variation in the data might result in a completely different tree being generated. <br>\n",
    "**Bagging** and **Boosting** are ensemble techniques to decrease tha variance in prediction and reduce bias respectively <br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "043c62a5",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "**Bootstrap Aggregating**, also known as **Bagging**, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. <br>\n",
    "It decreases the variance and helps to avoid overfitting.<br> \n",
    "It is a special case of the model averaging approach. <br>\n",
    "**STEPS**:\n",
    "- create a few subsets of data from the training data set, which is chosen randomly with replacement\n",
    "- prepare base Decision Trees in parallel with each training data subset independent of each other\n",
    "- ensemble of various models is obtained by picking the average output of all models ((30+23+44+21+50)/5 = 33.6)(regression) or more frequent class (Red, Blue, Red, Red, Blue = Red)(classification)\n",
    "\n",
    "![Bagging.png](images/Bagging.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8316c291",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. The basic idea behind boosting is converting many weak learners to form a single strong learner. <br>\n",
    "Boosting needs us to specify a weak model (e.g. regression, shallow decision trees, etc) and then improves it.\n",
    "In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm. <br>\n",
    "**STEPS**:\n",
    "- Firstly, a model is built from the training data (weak model)\n",
    "- Then the second model is built which tries to correct the errors present in the first model\n",
    "- This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added\n",
    "<br><br>\n",
    "\n",
    "Popular Boosting Algorithms : \n",
    "- AdaBoost or Adaptive Boosting for Classification problems which is implemented using iteratively refined sample weights\n",
    "- Gradient Boosting uses an internal regression model trained iteratively on the residuals\n",
    "- Extreme Gradient Boosting or XGBoost\n",
    "\n",
    "AdaBoost:\n",
    "![adaboost.png](images/adaboost.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "230400ac",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "Stacking is one of the popular ensemble modeling techniques in machine learning. Various weak learners are ensembled in a parallel manner in such a way that by combining them with Meta learners, we can predict better predictions for the future.\n",
    "\n",
    "![stacking.png](images/stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0791c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b250923",
   "metadata": {},
   "source": [
    "## Decision Tree using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e48041",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0efb7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8597ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "df = pd.read_csv(\"SUV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4705f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      User ID  Gender  Age  EstimatedSalary  Purchased\n",
      "188  15674206    Male   35            72000          0\n",
      "343  15629739  Female   47            51000          1\n",
      "166  15762228  Female   22            55000          0\n",
      "257  15794493    Male   40            57000          0\n",
      "(400, 5)\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "\n",
    "print(df.sample(4))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0daa857",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df.iloc[:, [1,2,3]].values, columns = ['sex', 'age', 'income'])\n",
    "y = pd.DataFrame(df.iloc[:, 4].values, columns = ['purchased'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71802873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9ca1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label = LabelEncoder()\n",
    "X['sex'] = label.fit_transform(X['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4679c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb6f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1f89d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Building / Decision Tree Building\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77c5f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9afacbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.85 \n",
      "Confusion Matrix :\n",
      " [[55  7]\n",
      " [ 8 30]]\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy :', accuracy, '\\nConfusion Matrix :\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395719c3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1671632",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88294113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "798fa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "data = pd.read_csv('winequality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "330045ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n",
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.085</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.62</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.092</td>\n",
       "      <td>8.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.186</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.99600</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.078</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.99625</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "62             7.5             0.520         0.16             1.9      0.085   \n",
       "253            7.7             0.775         0.42             1.9      0.092   \n",
       "303            7.4             0.670         0.12             1.6      0.186   \n",
       "231            8.0             0.380         0.06             1.8      0.078   \n",
       "\n",
       "     free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "62                  12.0                  35.0  0.99680  3.38       0.62   \n",
       "253                  8.0                  86.0  0.99590  3.23       0.59   \n",
       "303                  5.0                  21.0  0.99600  3.39       0.54   \n",
       "231                 12.0                  49.0  0.99625  3.37       0.52   \n",
       "\n",
       "     alcohol  quality  \n",
       "62       9.5        7  \n",
       "253      9.5        5  \n",
       "303      9.5        5  \n",
       "231      9.9        6  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA\n",
    "\n",
    "print(data.shape)\n",
    "print(list(data.columns))\n",
    "data.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaf66f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pd.DataFrame(data.iloc[:, :-1].values), pd.DataFrame(data.iloc[:,-1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60518cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52835961,  0.96187667, -1.39147228, ...,  1.28864292,\n",
       "        -0.57920652, -0.96024611],\n",
       "       [-0.29854743,  1.96744245, -1.39147228, ..., -0.7199333 ,\n",
       "         0.1289504 , -0.58477711],\n",
       "       [-0.29854743,  1.29706527, -1.18607043, ..., -0.33117661,\n",
       "        -0.04808883, -0.58477711],\n",
       "       ...,\n",
       "       [-1.1603431 , -0.09955388, -0.72391627, ...,  0.70550789,\n",
       "         0.54204194,  0.54162988],\n",
       "       [-1.39015528,  0.65462046, -0.77526673, ...,  1.6773996 ,\n",
       "         0.30598963, -0.20930812],\n",
       "       [-1.33270223, -1.21684919,  1.02199944, ...,  0.51112954,\n",
       "         0.01092425,  0.54162988]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91ea1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b09c310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model building / Decision Tree Regressor building\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2dd18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ad1eaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6375 \n",
      "R2 Score Matrix : 0.018193031144002503\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy :', accuracy, '\\nR2 Score Matrix :', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d626d4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d1fa7",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\n",
    "- https://towardsdatascience.com/decision-tree-in-machine-learning-e380942a4c96\n",
    "- https://medium.com/pursuitnotes/decision-tree-classification-in-9-steps-with-python-600c85ef56de\n",
    "- https://medium.com/pursuitnotes/decision-tree-regression-in-6-steps-with-python-1a1c5aa2ee16\n",
    "- https://towardsdatascience.com/boosting-the-accuracy-of-your-machine-learning-models-f878d6a2d185\n",
    "- https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/\n",
    "- https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
