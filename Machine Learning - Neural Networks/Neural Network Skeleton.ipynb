{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5b93a8",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "An **Artificial Neural Network (ANN)** is a computing system inspired by the biological neural networks that constitute the animal brain.<br><br>\n",
    "\n",
    "An ANN is based on a collection of connected units or nodes called artificial neurons bound in various layers which attempt to learn the underlying relationships in a set of data through various algorithms mimicing the way brain learns.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1639a5",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "\n",
    "\n",
    "1. Pick a **Neural Network Architecture** <br>\n",
    "\n",
    "2. Randomly initialize weights (or bias) in range [-Œµ, Œµ] \n",
    "where Œµ =$\\sqrt{6}$\t/ $\\sqrt{Lin + Lout}$ \t\n",
    "Lin, Lout = number of units in layer adjacent to $\\theta$\t<br>\n",
    "\n",
    "3. feed **input data** into the Neural Network <br>\n",
    "\n",
    "4. implement **Forward Propagation** to compute $‚Ñé\\_\\theta(ùë•)$  <br>\n",
    "the data flows **from layer to layer** until we have the output   <br>\n",
    "\n",
    "5. Adjust the given parameter (weight or bias) using **Backpropagation** by subtracting the derivative of the error with respect to the parameter itself\n",
    "\n",
    "6. iterate throught the process using Gradient Descent (or some other optimization algorithm) to minimize Loss/Cost\n",
    "\n",
    "The most important step is the 5th. We want to be able to have as many layers as we want, and of any type. But if we modify/add/remove one layer from the network, the output of the network is going to change, which is going to change the error, which is going to change the derivative of the error with respect to the parameters. We need to be able to compute the derivatives regardless of the network architecture, regardless of the activation functions, regardless of the loss we use.<br><br>\n",
    "\n",
    "In order to achieve that, we must implement **each layer separately**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b84cfd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fad8ac",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Every layer that we might create have at least 2 things in common: **input** and **output** data.    <br><br>\n",
    "\n",
    "![layer_common.png](images/layer_common.png)\n",
    "\n",
    "The output of one layer is the input of the next layer <br><br>\n",
    "\n",
    "![forward_propagation.png](images/forward_propagation.png)\n",
    "<br>\n",
    "This is **Forward Propagation**. <br><br>\n",
    "Essentially, the input data is fed into the first layer, then the ouput of every layer becomes the input of the next layer until we reach the end of the network. <br><br>\n",
    "\n",
    "We compute the outcome (hypotheses  $ h_\\theta(X))$ by feeding the input (X) with random initialized Weights (W) layer by layer forward into the Neural Network. <br><br>\n",
    "\n",
    "Next, we calculate the **Error : E** by feeding the computed values (**Y***) and actual values (**Y**) into an error function.<br>\n",
    "can calculate an error **E**. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864650ab",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "**Gradient Descent** is an iterative first order optimization algorithm to find a local minimum/local maximum of a given function.<br><br>\n",
    "\n",
    "**Gradient Descent** is used as an advanced optimization technique to minimize the error _E_ in Neural Networks \n",
    "\n",
    "_this is a quick reminder on Gradient Descent and not a learning resource_ <br><br>\n",
    "\n",
    "Basically, we change parameters (**W** and **B**) so as to decrease the error **E**:  \n",
    "\n",
    "$  W = W - \\alpha *\\frac{\\partial E}{\\partial W} $ <br>\n",
    "$  B = B - \\alpha *\\frac{\\partial E}{\\partial b} $\n",
    "\n",
    "where, $\\alpha$ is called learning rate which we manually set in the range [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65294e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865c51a",
   "metadata": {},
   "source": [
    "## Backward Propagation\n",
    "\n",
    "**Backward Propagation or Backpropagation** is an algorithm for supervised learning of artificial neural networks using gradient descent. <br><br>\n",
    "\n",
    "Given an Artificial Neural Network and an Error function, the method calculates the gradient of the error function with respect to the Neural Network's weights (W) i.e. $ \\frac{\\partial E}{\\partial W} $ which is further used in Gradient Descent to find the optimal values of W and B <br><br>\n",
    "\n",
    "Implementation: <br><br>\n",
    "\n",
    "Having caluclated the output **Y*** = $ h_\\theta(X) $ and the error **E** = error_function(Y*, Y), we can compute the derivative of the error w.r.t that layer's output $ \\frac{\\partial E}{\\partial Y} $. <br><br>\n",
    "\n",
    "Next, using the **Chain Rule**, we can compute \n",
    "1. the derivative of the error w.r.t the parameters $ \\frac{\\partial E}{\\partial W}, \\frac{\\partial E}{\\partial B} $ <br>\n",
    "2. the derivative of the error w.r.t the input $ \\frac{\\partial E}{\\partial X} $ (because as stated earlier, output of the current layer is the input of the next layer)\n",
    "\n",
    "![backpropagation_layer_by_layer.png](images/backpropagation_layer_by_layer.png)\n",
    "<br><br>\n",
    "\n",
    "**Formula** (_skipping the derivation_)\n",
    "\n",
    "![backpropagation_formula](images/backpropagation_formula.png)\n",
    "<br><br>\n",
    "**NOTE**: *E* is scalar (a number) and *X*, *Y*, *B* and *W* are matrices.<br><br>\n",
    "\n",
    "**NOTE**: ‚àÇE/‚àÇX needs to be calculated because since the output of the current layer acts as the input of the next layer, the derivative of error w.r.t the input for later layer acts as derivative of error w.r.t the output for previous layer (ex: ‚àÇE/‚àÇX of layer 3 = ‚àÇE/‚àÇY of layer 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c3b4a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efac90a",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "\n",
    "`layer`, `DenseLayer`, `ActivationLayer`, `activation_function` and `activation_function_derivative`, `cost_fun` and `cost_derivative` and `Network` classes/methods explained and defined below are written in separate python files and finally imported into the main file as modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567a8e6",
   "metadata": {},
   "source": [
    "## Abstract Class: Layer\n",
    "\n",
    "The abstract class *Layer*, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward propagation and backward propagation methods.<br><br>\n",
    "\n",
    "The `forward_propagation` method takes in the input and gives us the ouput **Y*** or **$h_\\theta(X)$**<br><br>\n",
    "\n",
    "The `backward_propagation` method takes in the derivate of the error w.r.t the output i.e. ‚àÇE/‚àÇY (output_grad) and is responsible for 2 things:\n",
    "1. updating the parameters (weights & bias) if any\n",
    "2. return the derivative of the error w.r.t the input i.e ‚àÇE/‚àÇX \n",
    "<br>\n",
    "\n",
    "NOTE: since `layer` is an Abstract class, `forward_propagation` and `backward_propagation` methods are only declared and not defined.<br>\n",
    "These methods are defined in the inherited classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc86057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abstract class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    #computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    #computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac78a02",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaa75f",
   "metadata": {},
   "source": [
    "## Dense Layer\n",
    "\n",
    "**Dense Layer** or **Fully Connected Layer** is a layer that is deeply connected with its preceding layer which means the neurons of the layer are connected to every neuron of its preceding layer.<br>\n",
    "In other words, every input neuron is connected to every output neuron<br>\n",
    "\n",
    "\n",
    "![fully_connected_layer.png](images/fully_connected_layer.png)\n",
    "\n",
    "The most basic purpose of a Dense layer is to **change the dimension of vector by using every neuron**<br><br>\n",
    "\n",
    "`Dense` inherits from the abstract class `layer` and defines the `forward_propagation` and `backward_propagation` methods\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "The value of each output neuron can be calculated as :\n",
    "$ Y = WX + B $\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "Given a matrix `output_gradient` containing the derivative of the error with respect to the layer's output i.e. $ \\frac{\\partial E}{\\partial Y} $, we compute:\n",
    "\n",
    "1. The derivative of the error w.r.t the parameters $ \\frac{\\partial E}{\\partial W} $, $ \\frac{\\partial E}{\\partial B} $\n",
    "\n",
    "2. The derivative of the error w.r.t the input $ \\frac{\\partial E}{\\partial X} $\n",
    "\n",
    "Upon derivation, we conclude:\n",
    "\n",
    "`weights_gradient`  $ \\frac{\\partial E}{\\partial W}     =  X^T \\frac{\\partial E}{\\partial Y}   $ \n",
    "\n",
    "`output_gradient `  $ \\frac{\\partial E}{\\partial B}     =  \\frac{\\partial E}{\\partial Y}       $\n",
    "\n",
    "`input_gradient `   $ \\frac{\\partial E}{\\partial X}     =   \\frac{\\partial E}{\\partial Y}  W^T $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93941db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Layer\n",
    "import numpy as np\n",
    "\n",
    "#inherit from base class Layer\n",
    "class DenseLayer(Layer):\n",
    "    #input_size = number of input neurons\n",
    "    #output_size = number of output neurons\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias    = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    #returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input  = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    #computes dE/dW, dE/dB for a given output_error(output_gradient)=dE/dY. Returns input_error(input_gradient)=dE/dX.\n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        input_gradient   = np.dot(output_gradient, self.weights.T)\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
    "\n",
    "        #update parameters\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias    -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efde32",
   "metadata": {},
   "source": [
    "### Activtion Layer\n",
    "\n",
    "The activation function is the most important factor in a neural network which decided whether or not a neuron will be activated or not and transferred to the next layer. <br>\n",
    "This simply means that it will decide whether the neuron's input to the network is relevant or not in the process of prediction.<br>\n",
    "\n",
    "Example : Binary Step Function, Linear Function, Sigmoid function, Tanh, etc<br><br>\n",
    "\n",
    "\n",
    "\n",
    "The Activation Layer simply takes in a group of neurons (a vector) and passes them through the activation function to give activated neurons (vector) of the same shape as the input neurons.<br>\n",
    "\n",
    "`ActivationLayer` takes 2 parameters : \n",
    "1. `activation`: activation function \n",
    "2. `activation_prime`: derivative of the activation function\n",
    "\n",
    "It has 2 methods:\n",
    "1. `forward_propagation` $ Y= f(X) $\n",
    "2. `backward_propagation` $ \\frac{\\partial E}{\\partial X} = \\frac{\\partial E}{\\partial Y} ‚äô f'(X) $ <br><br>\n",
    "\n",
    "**NOTE**: We don't use an Activation Function within the Dense Layer because it adds complicated calculations in the Dense Layer. <br>\n",
    "The Activation functions is just another layer, so it is implemented in it's own class.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39073087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layer import Layer\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    #returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input  = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    #returns input_error=dE/dX for a given output_gradient=dE/dY.\n",
    "    #learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979c2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def activation_function(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def activation_function_derivative(x):\n",
    "    return 1-np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937ff78",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "**Cost Function** measures the Error between the predicted value (Y*) and the actual value (Y).<br><br>\n",
    "\n",
    "There are many ways to define the error, and we use them as per the problem's need and/or the algorithm used.<br>\n",
    "We also need to define the derivative of the Cost Function.<br>\n",
    "\n",
    "One of the most known Cost Function is the **MSE ‚Äî Mean Squared Error** (which we use here)\n",
    "\n",
    "![mse.png](images/mse.png)\n",
    "Where **y*** and **y** denotes desired output and actual output respectively.<br><br>\n",
    "\n",
    "Now to define $ \\frac{\\partial E}{\\partial Y} $,\n",
    "\n",
    "![partial_derivative.png](images/partial_derivative.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1badbb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Cost function and its derivative\n",
    "def cost_fun(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def cost_derivative(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd73bcf",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfde4773",
   "metadata": {},
   "source": [
    "## Network Class\n",
    "\n",
    "`Network` defines methods to build the Neural Network, train it and predict output for given input<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3f94b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.cost   = None\n",
    "        self.cost_derivative = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        '''add layer to network (Dense/ActivationLayer)'''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def use(self, cost, cost_derivative):\n",
    "        '''set Cost function and it's derivative to use'''\n",
    "        self.cost = cost\n",
    "        self.cost_derivative = cost_derivative\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        '''predict output for given input'''\n",
    "        \n",
    "        #sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result  = []\n",
    "\n",
    "        #run network over all samples\n",
    "        for i in range(samples):\n",
    "            #forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        print(type(result))\n",
    "        return result\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        '''train the Neural Network'''\n",
    "\n",
    "        #epoch means one complete pass of the training dataset through the algorithm\n",
    "        #i.e. the number of times a learning algorithm sees the complete dataset.\n",
    "        \n",
    "        #sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        #training loop\n",
    "        for e in range(epochs):\n",
    "            err = 0\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                #forward propagation\n",
    "                output = x\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                #compute cost (for display purpose only)\n",
    "                err += self.cost(y, output)\n",
    "\n",
    "                #backpropagation\n",
    "                grad = self.cost_derivative(y, output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    grad = layer.backward_propagation(grad, learning_rate)\n",
    "\n",
    "            #calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (e+1, epochs, err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d7530",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28aaf8",
   "metadata": {},
   "source": [
    "## using the above Neural Network\n",
    "\n",
    "`layer`, <br>\n",
    "`DenseLayer`->Layer.py, <br>\n",
    "`ActivationLayer`->activationlayer.py, <br>\n",
    "`activation_function` and `activation_function_derivative`->activationfunctions.py, <br>\n",
    "`cost_fun` and `cost_derivative`->losses.py and <br>\n",
    "`Network`->network.py<br>\n",
    "classes/methods are written in aforementioned python files and imported into the following (main) file as modules for implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5860c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from network import Network\n",
    "from denselayer import DenseLayer\n",
    "from activationlayer import ActivationLayer\n",
    "from activationfunctions import activation_function, activation_function_derivative\n",
    "from losses import cost_fun, cost_derivative\n",
    "\n",
    "#training data\n",
    "X = input_data.reshape(4, 2, 1)\n",
    "Y = actual_ouput.reshape(4, 1, 1)\n",
    "\n",
    "#network\n",
    "net = Network()\n",
    "\n",
    "#net.add(DenseLayer(no_of_ip_neurons, no_of_op_neurons))\n",
    "#net.add(ActivationLayer(activation_function, activation_function_derivative))\n",
    "\n",
    "net.add(DenseLayer(2, 3)) \n",
    "net.add(ActivationLayer(activation_function, activation_function_derivative))\n",
    "net.add(DenseLayer(3, 1))\n",
    "net.add(ActivationLayer(activation_function, activation_function_derivative))\n",
    "\n",
    "#train\n",
    "net.use(cost_fun, cost_derivative)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "#test\n",
    "output = net.predict(x_train)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f1451",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212fdbd3",
   "metadata": {},
   "source": [
    "for better understanding on how to use the modules, refer **example_xor.py** in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4ed9c",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning/home/welcome)\n",
    "- [Neural Network from scratch in Python - article](https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65)\n",
    "- [Neural Network from scratch in Python - video tutorial](https://www.youtube.com/watch?v=pauPCy_s0Ok&t=2s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
