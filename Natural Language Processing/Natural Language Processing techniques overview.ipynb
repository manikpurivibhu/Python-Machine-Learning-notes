{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32612189",
   "metadata": {},
   "source": [
    "# 1. Bag of Words\n",
    "\n",
    "<div id=\"bagOfWords\"> <br>\n",
    "    \n",
    "**Bag of Words** is a Natural Language Processing technique of text modeling to convert raw text into numerical feature vectors. This model can be visualized using a table, which contains the count of words corresponding to the word itself, or 0/1 for binary events for probabilistic models. <br>\n",
    "\n",
    "The most common ways to extract numerical features from text content are:\n",
    "\n",
    "- **Tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators. Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.\n",
    "- **Counting** the occurrences of tokens in each document.\n",
    "- **Normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "This specific strategy (tokenization, counting and normalization) is called the **Bag of Words**. <br>\n",
    "\n",
    "**Vectorization** is the general process of turning a collection of text documents into numerical feature extractions. <br>m\n",
    " \n",
    "A corpus (collection of authentic text or audio organized into datasets) of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507a540",
   "metadata": {},
   "source": [
    "#### Limitations of Bag of Words\n",
    "\n",
    "- For large documents, the resultant vectors will be of large dimension and will contain far too many null values resulting in sparse vectors. \n",
    "- Bag of Words model completely ignores the grammar and semantics of raw text. It would generate separate vectors for \"Book\" and \"Books\" and \"this is good\" and \"is this good\" are of equal weightage to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680148e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bed4575f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenized text : \n",
      " ['beans ', 'i was trying to explain to somebody as we were flying in that s corn ', 'that s beans ']\n",
      "Dictionary of Bag of Words :\n",
      "a : 2\n",
      "addressing : 1\n",
      "again : 1\n",
      "aged : 1\n",
      "\n",
      "9 Most frequent words :  ['your', 'you', 'wondering', 'with', 'why', 'who', 'were', 'we', 'way']\n",
      "Numerical Feature Vectors :\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "## implementing Bag of Words with numpy and nltk\n",
    "\n",
    "text = \"Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I haven’t seen in a long time, and somehow he has not aged and I have. And it’s great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didn’t speak at the commencement.\"\n",
    "\n",
    "# Tokenize\n",
    "dataset = nltk.sent_tokenize(text)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n",
    "print(\"Sentence tokenized text : \\n\", dataset[:3])\n",
    "\n",
    "# counting: obtain the most frequent words in text\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    # tokenize each sentence to words\n",
    "    words = nltk.word_tokenize(data)\n",
    "    # words counter dictionary\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "temp = sorted(word2count.items())[:4]\n",
    "sentence = \"\"\n",
    "for pair in temp:\n",
    "    sentence += str(pair[0]) + \" : \" + str(pair[1]) + \"\\n\"\n",
    "print(\"Dictionary of Bag of Words :\\n\" + sentence)\n",
    "\n",
    "# normalize (here): select a particular number of most frequent words\n",
    "import heapq\n",
    "freq_words = heapq.nlargest(100, word2count)\n",
    "print(\"9 Most frequent words : \", freq_words[:9])\n",
    "\n",
    "# vectorize\n",
    "X = []\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)\n",
    "X = np.asarray(X)\n",
    "print(\"Numerical Feature Vectors :\\n\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5839286c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['addressing' 'again' 'aged' 'agricultural' 'also' 'am' 'amaury' 'and'\n",
      " 'are' 'as' 'at' 'award' 'be' 'beans' 'being' 'bunch' 'by' 'commencement'\n",
      " 'corn' 'country' 'deeply' 'dick' 'didn' 'douglas' 'durbin' 'edgar'\n",
      " 'elephant' 'everybody' 'explain' 'finest' 'for' 'governor' 'have' 'he'\n",
      " 'here' 'impressed' 'in' 'including' 'introduction' 'is' 'it' 'killeen'\n",
      " 'know' 'knowledge' 'long' 'lucky' 'making' 'me' 'much' 'my' 'not'\n",
      " 'noticed' 'now' 'of' 'once' 'one' 'outstanding' 'path' 'paul' 'people'\n",
      " 'please' 'possible' 'president' 'public' 're' 'room' 'see' 'seen'\n",
      " 'senator' 'senators' 'served' 'service' 'set' 'so' 'somebody' 'somehow'\n",
      " 'speak' 'start' 'still' 'system' 'thank' 'that' 'the' 'they' 'time' 'to'\n",
      " 'today' 'trying' 'up' 'very' 'want' 'was' 'way' 'we' 'were' 'who' 'why'\n",
      " 'with' 'wondering' 'you']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "## implementing Bag of Words using sklearn.feature_exttraction module\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "dataset = nltk.sent_tokenize(text)\n",
    "vectorizer = CountVectorizer(max_features=100)\n",
    "vectors = vectorizer.fit_transform(dataset)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f04e4d",
   "metadata": {},
   "source": [
    "### Unigram, bigram, n-gram models\n",
    "Unigram, bigram and n-gram sentences are 1-word, 2-word or n-word sentences respectively. Unigram, bigram, n-gram models make use of 1, 2 or n-word sentences to create vectors. <br>\n",
    "\"I do not like the book\" will have [\"I\", \"do\", \"not\", \"like\", \"the\", \"book\"] and [\"I do\", \"do not\", \"not like\", \"like the\", \"the book\"] in unigram and bigram models respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c71d9c",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- <a href=\"https://www.youtube.com/watch?v=M7SWr5xObkA&list=WL&index=2\">Natural Language Processing tutorial - YouTube </a>\n",
    "- <a href=\"https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/\">Bag of words (BoW) model in NLP - Geeksforgeeks </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
